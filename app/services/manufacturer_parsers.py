"""
–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –ø–ª–∏—Ç–∫–∏.
–ö–∞–∂–¥—ã–π –ø–∞—Ä—Å–µ—Ä –∑–Ω–∞–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç–∞ —Å–≤–æ–µ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è.
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from abc import ABC, abstractmethod
import re
from urllib.parse import urljoin, urlparse
import os
import hashlib
from werkzeug.utils import secure_filename
import time  # –î–ª—è –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏


class BaseManufacturerParser(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è"""
    
    def __init__(self, base_url: str, slug: str):
        self.base_url = base_url
        self.slug = slug
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def fetch_page(self, url: str) -> Optional[BeautifulSoup]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            print(f"  üìÑ –ó–∞–≥—Ä—É–∑–∫–∞: {url}")
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {str(e)}")
            return None
    
    def normalize_url(self, url: str) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π"""
        if not url:
            return ''
        return urljoin(self.base_url, url)
    
    def download_image(self, image_url: str, retry_count: int = 3) -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫."""
        if not image_url:
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –≤–∞–ª–∏–¥–µ–Ω
        if not image_url.startswith('http'):
            print(f"  ‚ö†Ô∏è  –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_url}")
            return None
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        url_hash = hashlib.md5(image_url.encode()).hexdigest()[:10]
        ext = os.path.splitext(urlparse(image_url).path)[1] or '.jpg'
        # –£–±–∏—Ä–∞–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        ext = ext.split('?')[0]
        if not ext or len(ext) > 5:
            ext = '.jpg'
        filename = f"{self.slug}_{url_hash}{ext}"
        
        # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        upload_dir = os.path.join('app', 'static', 'uploads', 'manufacturers')
        os.makedirs(upload_dir, exist_ok=True)
        
        filepath = os.path.join(upload_dir, filename)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∞–Ω–æ –ª–∏ —É–∂–µ
        if os.path.exists(filepath):
            print(f"  ‚ÑπÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {filename}")
            return f'manufacturers/{filename}'
        
        # –ü—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        for attempt in range(retry_count):
            try:
                if attempt > 0:
                    print(f"  üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retry_count}...")
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                print(f"  ‚¨áÔ∏è  –°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {image_url[:80]}...")
                response = requests.get(image_url, headers=self.headers, timeout=20, stream=True)
                response.raise_for_status()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content_type = response.headers.get('content-type', '')
                if 'image' not in content_type:
                    print(f"  ‚ö†Ô∏è  –ù–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {content_type}")
                    return None
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π (–æ–±—ã—á–Ω–æ < 5KB —ç—Ç–æ thumbnail –∏–ª–∏ —Å–ª–æ–º–∞–Ω–Ω—ã–π —Ñ–∞–π–ª)
                # –ê —Ç–∞–∫–∂–µ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (> 3MB —ç—Ç–æ –æ–±—ã—á–Ω–æ –Ω–µ–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
                file_size = os.path.getsize(filepath)
                min_size = 5000      # –ú–∏–Ω–∏–º—É–º 5KB
                max_size = 20000000  # –ú–∞–∫—Å–∏–º—É–º 20MB
                
                if file_size < min_size:
                    print(f"  ‚ö†Ô∏è  –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π ({file_size} bytes), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    os.remove(filepath)
                    if attempt < retry_count - 1:
                        continue
                    return None
                
                if file_size > max_size:
                    print(f"  ‚ö†Ô∏è  –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file_size / 1000000:.1f} MB), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    os.remove(filepath)
                    if attempt < retry_count - 1:
                        continue
                    return None
                
                print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename} ({os.path.getsize(filepath)} bytes)")
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
                return f'manufacturers/{filename}'
                
            except requests.exceptions.Timeout as e:
                print(f"  ‚è±Ô∏è  –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {str(e)}")
                if attempt < retry_count - 1:
                    continue
                return None
            except requests.exceptions.RequestException as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {image_url}: {str(e)}")
                if attempt < retry_count - 1:
                    continue
                return None
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
                if attempt < retry_count - 1:
                    continue
                return None
        
        return None
    
    def extract_logo(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ª–æ–≥–æ—Ç–∏–ø –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        print(f"üîç –ü–æ–∏—Å–∫ –ª–æ–≥–æ—Ç–∏–ø–∞ –¥–ª—è {self.slug}...")
        
        soup = self.fetch_page(self.base_url)
        if not soup:
            return None
        
        # –ò—â–µ–º –ª–æ–≥–æ—Ç–∏–ø –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        logo = None
        
        # 1. –ü–æ alt –∞—Ç—Ä–∏–±—É—Ç—É
        logo_img = soup.find('img', {'alt': re.compile(r'logo', re.I)})
        if logo_img:
            logo = logo_img.get('src') or logo_img.get('data-src')
        
        # 2. –ü–æ –∫–ª–∞—Å—Å—É
        if not logo:
            logo_img = soup.find('img', class_=re.compile(r'logo', re.I))
            if logo_img:
                logo = logo_img.get('src') or logo_img.get('data-src')
        
        # 3. –í header navbar
        if not logo:
            header = soup.find(['header', 'nav'], class_=re.compile(r'navbar|header', re.I))
            if header:
                logo_img = header.find('img')
                if logo_img:
                    logo = logo_img.get('src') or logo_img.get('data-src')
        
        if logo:
            logo_url = self.normalize_url(logo)
            # –°–∫–∞—á–∏–≤–∞–µ–º –ª–æ–≥–æ—Ç–∏–ø
            local_path = self.download_image(logo_url)
            return local_path
        
        print(f"  ‚ö†Ô∏è  –õ–æ–≥–æ—Ç–∏–ø –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    @abstractmethod
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å —Å–∞–π—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è"""
        pass
    
    @abstractmethod
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        pass
    
    @abstractmethod
    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã —Å —Å–∞–π—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è"""
        pass
    
    @abstractmethod
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ —Å —Å–∞–π—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è"""
        pass


class ApariciParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Aparici (https://www.aparici.com/de)"""
    
    def __init__(self):
        super().__init__('https://www.aparici.com/de', 'aparici')
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Aparici"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Aparici...")
        
        soup = self.fetch_page(f"{self.base_url}/kollektionen")
        if not soup:
            return []
        
        collections = []
        
        # Aparici: –∏—â–µ–º —Å—Å—ã–ª–∫–∏ —Å –∫–ª–∞—Å—Å–æ–º sub-menu_colecciones-item
        collection_links = soup.find_all('a', class_='sub-menu_colecciones-item')
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ - –∏—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å href —Å–æ–¥–µ—Ä–∂–∞—â–∏–º /kollektionen/
        if not collection_links:
            all_links = soup.find_all('a', href=True)
            collection_links = [link for link in all_links if '/kollektionen/' in link.get('href', '') and link.get('href', '') != f'{self.base_url}/kollektionen']
        
        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(collection_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
        
        for idx, link in enumerate(collection_links[:20], 1):  # –£–≤–µ–ª–∏—á–∏–ª –ª–∏–º–∏—Ç –¥–æ 20
            collection_url = self.normalize_url(link.get('href', ''))
            if not collection_url or collection_url == f'{self.base_url}/kollektionen':
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏
            title = link.get_text(strip=True)
            
            if not title or len(title) < 2:
                continue
            
            print(f"  üîó –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {idx}/{len(collection_links[:20])}: {title}")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            detail = self.extract_collection_detail(collection_url)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–∫–∞—á–∏–≤–∞–µ–º –µ–≥–æ
            local_image_path = None
            if detail.get('image_url'):
                local_image_path = self.download_image(detail['image_url'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if not local_image_path and not detail.get('image_url'):
                print(f"  ‚ö†Ô∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è {title} –ø—Ä–æ–ø—É—â–µ–Ω–∞ - –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                continue
            
            collections.append({
                'title': title,
                'description': detail.get('description', ''),
                'full_content': detail.get('full_content', ''),
                'technical_specs': detail.get('technical_specs', ''),
                'image_url': local_image_path or detail.get('image_url'),
                'source_url': collection_url
            })
            
            print(f"  ‚úì –ö–æ–ª–ª–µ–∫—Ü–∏—è {title} –¥–æ–±–∞–≤–ª–µ–Ω–∞")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–µ—Ä–≤–µ—Ä
            if idx < len(collection_links[:20]):
                time.sleep(0.5)  # 500ms –∑–∞–¥–µ—Ä–∂–∫–∞
        
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Aparici"""
        soup = self.fetch_page(url)
        if not soup:
            return {}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏)
        h1 = soup.find('h1', class_='s-headerCorporate_title')
        title = h1.get_text(strip=True) if h1 else ''
        
        # –ò—â–µ–º –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        # –í Aparici —ç—Ç–æ –æ–±—ã—á–Ω–æ –±–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∫–ª–∞—Å—Å–æ–º img-fluid –∏ alt —Ä–∞–≤–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—é –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        main_image = None
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å alt —Ä–∞–≤–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—é –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–º –µ–≥–æ
        images = soup.find_all('img', class_='img-fluid')
        for img in images:
            alt = img.get('alt', '').lower()
            src = img.get('src') or img.get('data-src')
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–æ–≥–æ—Ç–∏–ø—ã
            if 'logo' in alt or 'logo' in (src or '').lower():
                continue
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º alt
            if alt and len(alt) > 2 and '_big' in (src or '') or 'rect' in (src or ''):
                main_image = self.normalize_url(src)
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –±–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if not main_image:
            for img in images:
                src = img.get('src') or img.get('data-src')
                if src and 'logo' not in src.lower():
                    main_image = self.normalize_url(src)
                    break
        
        # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        description = ""
        # –í Aparici –æ–ø–∏—Å–∞–Ω–∏—è –æ–±—ã—á–Ω–æ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏
        desc_elem = soup.find('div', class_=re.compile(r'description|intro|summary', re.I))
        if desc_elem:
            paragraphs = desc_elem.find_all('p')
            if paragraphs:
                desc_texts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
                    if text and len(text) > 30 and 'brauche hilfe' not in text.lower() and 'einloggen' not in text.lower():
                        desc_texts.append(text)
                if desc_texts:
                    description = ' '.join(desc_texts[:2])[:300]
        
        # –ò—â–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        specs = ""
        # –í Aparici –º–æ–≥—É—Ç –±—ã—Ç—å –±–ª–æ–∫–∏ —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ –∏ –æ—Ç–¥–µ–ª–∫–æ–π
        specs_sections = soup.find_all(['div', 'section'], class_=re.compile(r'formato|acabado|caracteristica|specs', re.I))
        if specs_sections:
            specs_items = []
            for section in specs_sections[:3]:
                # –ò—â–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                items = section.find_all(['li', 'p', 'span'], limit=20)
                for item in items:
                    text = item.get_text(strip=True)
                    if text and 10 < len(text) < 100:
                        specs_items.append(text)
            if specs_items:
                specs = "\n".join(specs_items[:15])
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç - –±–µ—Ä–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        content_parts = []
        product_images = soup.find_all('img', {'alt': title})
        for img in product_images[:8]:
            src = img.get('src') or img.get('data-src')
            if src and 'logo' not in src.lower():
                img_url = self.normalize_url(src)
                content_parts.append(f'<img src="{img_url}" alt="{title}" class="img-fluid mb-3">')
        
        return {
            'description': description or f'Kollektion {title}',
            'full_content': '\n'.join(content_parts),
            'technical_specs': specs,
            'image_url': main_image
        }
    
    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã Aparici"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ Aparici...")
        
        soup = self.fetch_page(f"{self.base_url}/projekte")
        if not soup:
            return []
        
        projects = []
        
        # Aparici: –ø—Ä–æ–µ–∫—Ç—ã –≤ div —Å –∫–ª–∞—Å—Å–æ–º e_projectList
        project_cards = soup.find_all('div', class_='e_projectList')
        
        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(project_cards)} –ø—Ä–æ–µ–∫—Ç–æ–≤")
        
        for idx, card in enumerate(project_cards[:20], 1):
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏
            link = card.find('a', class_='e_projectList-container')
            if not link:
                link = card.find('a')
            
            if not link:
                continue
            
            project_url = self.normalize_url(link.get('href', ''))
            if not project_url:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ - —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ –∏–ª–∏ h-—ç–ª–µ–º–µ–Ω—Ç
            title = ''
            title_elem = card.find(['h2', 'h3', 'h4', 'h5'])
            if title_elem:
                title = title_elem.get_text(strip=True)
            else:
                # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Å—Å—ã–ª–∫–∏, –æ—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–µ–≥–æ
                title_text = link.get_text(strip=True)
                # –£–±–∏—Ä–∞–µ–º —á–∞—Å—Ç–∏ —Ç–∏–ø–∞ "Restaurants|Spain"
                if '|' in title_text:
                    parts = title_text.split('|')
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å (–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞)
                    title = parts[-1].strip() if len(parts) > 1 else title_text
                else:
                    title = title_text
            
            if not title or len(title) < 3:
                continue
            
            print(f"  üîó –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ {idx}/{len(project_cards[:20])}: {title[:50]}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = card.find('img', class_='e_projectList-image')
            if not img:
                img = card.find('img')
            
            image_url = None
            local_image_path = None
            if img:
                image_url = self.normalize_url(img.get('src') or img.get('data-src') or img.get('data-lazy', ''))
                if image_url:
                    local_image_path = self.download_image(image_url)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–µ–∫—Ç—ã –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if not local_image_path and not image_url:
                print(f"  ‚ö†Ô∏è  –ü—Ä–æ–µ–∫—Ç {title} –ø—Ä–æ–ø—É—â–µ–Ω - –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
            description = ''
            desc_elem = card.find('div', class_='e_projectList-content')
            if desc_elem:
                description = desc_elem.get_text(strip=True)[:300]
            
            projects.append({
                'title': title,
                'description': description,
                'full_content': '',
                'image_url': local_image_path or image_url,
                'source_url': project_url
            })
            
            print(f"  ‚úì –ü—Ä–æ–µ–∫—Ç {title[:50]} –¥–æ–±–∞–≤–ª–µ–Ω")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if idx < len(project_cards[:20]):
                time.sleep(0.3)
        
        return projects
    
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ Aparici"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ Aparici...")
        
        soup = self.fetch_page(f"{self.base_url}/blog")
        if not soup:
            return []
        
        blog_posts = []
        
        # Aparici: —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ –≤ div —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∫–ª–∞—Å—Å–∞ e_noticiaList-image
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ (–Ω–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
        all_links = soup.find_all('a', href=True)
        blog_article_links = []
        
        for link in all_links:
            href = link.get('href', '')
            # –§–∏–ª—å—Ç—Ä—É–µ–º: –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–º–µ—é—Ç length=6, —Å—Ç–∞—Ç—å–∏ length=7+
            if '/blog/' in href and len(href.split('/')) > 6:
                if href not in [bl.get('href') for bl in blog_article_links]:
                    blog_article_links.append(link)
        
        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(blog_article_links)} —Å—Ç–∞—Ç–µ–π –±–ª–æ–≥–∞")
        
        for idx, link in enumerate(blog_article_links[:15], 1):
            article_url = self.normalize_url(link.get('href', ''))
            if not article_url:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = link.get_text(strip=True)
            
            # –û—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –¥–∞—Ç
            if title:
                # –£–±–∏—Ä–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –¥–∞—Ç—É –≤ –Ω–∞—á–∞–ª–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä "Trends23 Januar 2026")
                import re
                # –ò—â–µ–º –¥–∞—Ç—É –∏ —É–±–∏—Ä–∞–µ–º –≤—Å—ë –¥–æ –Ω–µ—ë
                date_match = re.search(r'\d{1,2}\s+\w+\s+\d{4}', title)
                if date_match:
                    title = title[date_match.end():].strip()
            
            if not title or len(title) < 5:
                continue
            
            print(f"  üîó –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ {idx}/{len(blog_article_links[:15])}: {title[:50]}")
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ - –æ–Ω–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤–Ω—É—Ç—Ä–∏ –∏–ª–∏ —Ä—è–¥–æ–º —Å —Å—Å—ã–ª–∫–æ–π
            parent = link.parent
            img = None
            
            # –ò—â–µ–º –≤ —Å–∞–º–æ–π —Å—Å—ã–ª–∫–µ
            img = link.find('img', class_='e_noticiaList-image')
            
            # –ï—Å–ª–∏ –Ω–µ—Ç, –∏—â–µ–º –≤ —Ä–æ–¥–∏—Ç–µ–ª–µ
            if not img and parent:
                img = parent.find('img', class_='e_noticiaList-image')
            
            # –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë –Ω–µ—Ç, –∏—â–µ–º –ª—é–±–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä—è–¥–æ–º
            if not img and parent:
                img = parent.find('img')
            
            image_url = None
            local_image_path = None
            if img:
                image_url = self.normalize_url(img.get('src') or img.get('data-src') or img.get('data-lazy', ''))
                if image_url:
                    local_image_path = self.download_image(image_url)
            
            # –î–ª—è –±–ª–æ–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
            if not local_image_path and not image_url:
                print(f"  ‚ö†Ô∏è  –°—Ç–∞—Ç—å—è {title} –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            
            blog_posts.append({
                'title': title,
                'content': '',  # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                'full_content': '',
                'image_url': local_image_path or image_url,
                'source_url': article_url
            })
            
            print(f"  ‚úì –°—Ç–∞—Ç—å—è {title[:50]} –¥–æ–±–∞–≤–ª–µ–Ω–∞")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if idx < len(blog_article_links[:15]):
                time.sleep(0.3)
        
        return blog_posts


class DuneParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Dune Ceramics"""
    
    def __init__(self):
        super().__init__('https://duneceramics.com/de', 'dune')
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Dune"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Dune...")

        soup = self.fetch_page(f"{self.base_url}/serien")
        if not soup:
            return []

        collections = []
        seen = set()

        # Dune uses /serien/<slug> (and /series for other langs)
        links = soup.find_all('a', href=re.compile(r'/(serien|series)/'))
        for a in links:
            href = a.get('href')
            if not href:
                continue
            full = self.normalize_url(href)
            if full in seen:
                continue
            seen.add(full)

            title = a.get_text(strip=True)
            if not title or len(title) < 2:
                # try nearby heading
                h = a.find_previous(['h2', 'h3'])
                title = h.get_text(strip=True) if h else full.rstrip('/').split('/')[-1].replace('-', ' ').title()

            # try to get image from the link or parent
            img = a.find('img')
            if not img:
                parent = a.find_parent(['div', 'li', 'figure', 'article'])
                if parent:
                    img = parent.find('img')

            image_path = None
            image_url = None
            if img:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy')
                if src and not src.startswith('data:'):
                    image_url = self.normalize_url(src)
                    image_path = self.download_image(image_url)

            detail = self.extract_collection_detail(full) or {}

            # Prefer detail page images (they are more specific), fall back to link/thumbnail
            chosen_image = None
            detail_images = detail.get('images') or []
            if detail_images:
                chosen_image = detail_images[0]
            else:
                chosen_image = image_path or image_url

            collections.append({
                'title': title,
                'description': detail.get('description', ''),
                'full_content': detail.get('full_content', ''),
                'technical_specs': detail.get('technical_specs', ''),
                'image_url': chosen_image,
                'source_url': full
            })

        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Dune"""
        soup = self.fetch_page(url)
        if not soup:
            return {}
        description = ""
        content_parts = []

        desc_elem = soup.find('div', class_=re.compile(r'description|intro|text|content', re.I))
        if desc_elem:
            description = desc_elem.get_text(strip=True)[:300]
            for p in desc_elem.find_all('p', limit=6):
                t = p.get_text(strip=True)
                if t:
                    content_parts.append(f"<p>{t}</p>")

        # Collect image URLs from the detail page, deduplicate and prefer images
        images = []
        candidates = []
        parsed_path = urlparse(url).path.rstrip('/')
        slug = parsed_path.split('/')[-1].lower() if parsed_path else ''

        # Nav/menu category keywords to exclude (these appear in navigation and repeat often)
        nav_keywords = ['pavimentos', 'revestimientos', 'mosaicos', 'lavabos', 'sanitarios', 
                        'ceramica', 'porcelanico']  # Generic category names
        
        # Remove nav/footer/header from consideration
        nav_header = soup.find(['nav', 'header', 'footer'])
        if nav_header:
            for elem in nav_header.find_all('img'):
                elem.decompose()

        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if not src:
                continue
            # normalize and skip data URLs and small site assets
            if src.startswith('data:'):
                continue
            norm = self.normalize_url(src)
            low = norm.lower()
            
            # Skip logos and site navigation assets
            if 'logo' in low:
                continue
            if '/assets/images/' in low:
                continue
            
            # Skip known nav category images (pavimentos, revestimientos, mosaicos)
            # These repeat and are not collection-specific
            if any(kw in low for kw in nav_keywords):
                # Check if it's a specific collection image (contains slug) or generic category
                if not (slug and slug in low):
                    # This looks like a generic category image, skip it
                    continue
            
            candidates.append(norm)

        # Deduplicate while preserving order
        seen_urls = set()
        unique = []
        for c in candidates:
            if c in seen_urls:
                continue
            seen_urls.add(c)
            unique.append(c)

        # Prefer images that include the collection slug or contain 'serie'/'amb' (ambient/product images)
        def score_image(u: str) -> int:
            lu = u.lower()
            # Collection-specific (highest priority)
            if slug and slug in lu:
                return 0
            # Ambient/product images
            if 'amb-' in lu:
                return 1
            # Series images
            if 'serie' in lu or 'series' in lu:
                return 2
            # Fallback - generic product-related
            return 3

        unique.sort(key=score_image)

        # Download up to 8 preferred images
        for src in unique:
            p = self.download_image(src)
            if p:
                images.append(p)
            if len(images) >= 8:
                break

        return {
            'description': description,
            'full_content': '\n'.join(content_parts),
            'technical_specs': '',
            'images': images
        }
    
    def extract_projects(self) -> List[Dict]:
        return []
    
    def extract_blog_posts(self) -> List[Dict]:
        posts = []
        # Dune exposes a blog at /blog
        candidates = [f"{self.base_url}/blog", f"{self.base_url}/de/blog", f"{self.base_url}/en/blog"]
        soup = None
        for c in candidates:
            soup = self.fetch_page(c)
            if soup:
                break
        if not soup:
            return posts

        seen = set()
        # common article selectors
        for sel in ['article', '[class*=post]', '[class*=blog]', 'div.card', 'div.post-item']:
            elems = soup.select(sel)
            if not elems:
                continue
            for el in elems:
                a = el.find('a', href=True)
                if not a:
                    continue
                full = self.normalize_url(a.get('href'))
                if full in seen:
                    continue
                seen.add(full)

                title = (el.find(['h1', 'h2', 'h3']) and el.find(['h1', 'h2', 'h3']).get_text(strip=True)) or a.get_text(strip=True)
                img = el.find('img') or a.find('img')
                image = None
                if img:
                    src = img.get('src') or img.get('data-src')
                    if src and 'logo' not in src:
                        image = self.download_image(self.normalize_url(src))

                posts.append({'title': title, 'url': full, 'image_url': image, 'excerpt': ''})
            if posts:
                break

        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(posts)}")
        return posts


class EquipeParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Equipe Ceramicas"""
    
    def __init__(self):
        super().__init__('https://www.equipeceramicas.com/de', 'equipe')
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Equipe"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Equipe (–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ)...")

        collections = []
        visited = set()

        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π
        start_url = f"{self.base_url}/kollektionen"
        page = 1
        while True:
            url = start_url if page == 1 else f"{start_url}/page/{page}/"
            soup = self.fetch_page(url)
            if not soup:
                break

            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ (portfolio-item)
            links = soup.find_all('a', href=re.compile(r'/portfolio-item/'))
            if not links:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ –∏—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –∫–ª–∞—Å—Å–æ–º portfolio
                links = [a for a in soup.find_all('a', href=True) if '/portfolio-item/' in a.get('href', '')]

            for a in links:
                href = a.get('href')
                if not href:
                    continue
                full = self.normalize_url(href)
                if full in visited:
                    continue
                visited.add(full)

                # –ó–∞–≥–æ–ª–æ–≤–æ–∫: —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ –∏–ª–∏ –±–ª–∏–∂–∞–π—à–∏–π <h3>/<h2>
                title = a.get_text(strip=True)
                if not title:
                    h = a.find_previous(['h2', 'h3', 'h4'])
                    title = h.get_text(strip=True) if h else full.rstrip('/').split('/')[-1].replace('-', ' ').title()

                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –≤–Ω—É—Ç—Ä–∏ —Å—Å—ã–ª–∫–∏ –∏–ª–∏ –≤ —Ä–æ–¥–∏—Ç–µ–ª–µ
                img = a.find('img')
                if not img:
                    parent = a.find_parent(['div', 'article', 'figure'])
                    if parent:
                        img = parent.find('img')

                image_path = None
                if img:
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy')
                    if src and not src.startswith('data:'):
                        image_path = self.download_image(self.normalize_url(src))

                # –°–æ–±–∏—Ä–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                detail = self.extract_collection_detail(full) or {}

                collections.append({
                    'title': title,
                    'description': detail.get('description', ''),
                    'full_content': detail.get('full_content', ''),
                    'technical_specs': detail.get('technical_specs', ''),
                    'image_url': image_path or detail.get('images', [None])[0],
                    'source_url': full
                })

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é: –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            pager = soup.find('a', href=re.compile(r'/kollektionen/page/\d+/'))
            if pager:
                page += 1
                time.sleep(0.3)
                continue
            break

        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        soup = self.fetch_page(url)
        if not soup:
            return {}

        description = ""
        content_parts = []

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        h1 = soup.find(['h1', 'h2'])
        title = h1.get_text(strip=True) if h1 else url.rstrip('/').split('/')[-1]

        # –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑ meta –∏–ª–∏ –ø–µ—Ä–≤—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        meta = soup.find('meta', {'name': 'description'}) or soup.find('meta', {'property': 'og:description'})
        if meta and meta.get('content'):
            description = meta.get('content')[:300]
        else:
            main = soup.find(['main', 'article', 'div'], class_=re.compile(r'content|portfolio|single', re.I))
            if main:
                for p in main.find_all('p', limit=6):
                    text = p.get_text(strip=True)
                    if len(text) > 30:
                        if not description:
                            description = text[:300]
                        content_parts.append(f"<p>{text}</p>")

        # –°–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (wp uploads)
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and ('wp-content' in src or 'uploads' in src):
                p = self.download_image(self.normalize_url(src))
                if p:
                    images.append(p)
            if len(images) >= 8:
                break

        return {
            'title': title,
            'description': description,
            'full_content': '\n'.join(content_parts),
            'technical_specs': '',
            'images': images
        }

    def extract_projects(self) -> List[Dict]:
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ Equipe...")
        projects = []
        soup = self.fetch_page(f"{self.base_url}/projekte")
        if not soup:
            return projects

        links = soup.find_all('a', href=re.compile(r'/portfolio-item/'))
        seen = set()
        for a in links[:200]:
            href = a.get('href')
            if not href:
                continue
            full = self.normalize_url(href)
            if full in seen:
                continue
            seen.add(full)

            title = a.get_text(strip=True) or a.find_previous(['h2', 'h3']).get_text(strip=True)
            img = a.find('img') or a.find_parent().find('img') if a.find_parent() else None
            image_path = None
            if img:
                src = img.get('src') or img.get('data-src')
                if src:
                    image_path = self.download_image(self.normalize_url(src))

            projects.append({'title': title, 'description': '', 'full_content': '', 'image_url': image_path, 'source_url': full})

        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–µ–∫—Ç–æ–≤: {len(projects)}")
        return projects
    
    def extract_projects(self) -> List[Dict]:
        return []
    
    def extract_blog_posts(self) -> List[Dict]:
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π Equipe...")
        posts = []

        # Try common blog/news paths; prefer /blog/ or /news/
        candidates = [f"{self.base_url}/blog/", f"{self.base_url}/news/", f"{self.base_url}/de/news/"]
        soup = None
        for c in candidates:
            soup = self.fetch_page(c)
            if soup:
                base_page = c
                break
        if not soup:
            return posts

        # Find article elements (article, .post, .news, .entry)
        article_selectors = ['article', '[class*=post]', '[class*=news]', '[class*=entry]', 'div.card', 'div.blog-item']
        seen = set()

        for sel in article_selectors:
            elems = soup.select(sel)
            if not elems:
                continue
            for el in elems:
                a = el.find('a', href=True)
                if not a:
                    continue
                href = a.get('href')
                if not href:
                    continue
                full = self.normalize_url(href)
                if full in seen:
                    continue
                seen.add(full)

                title = (el.find(['h1', 'h2', 'h3']) and el.find(['h1', 'h2', 'h3']).get_text(strip=True)) or a.get_text(strip=True) or ''
                img = el.find('img') or a.find('img') or el.find_previous('img')
                image_path = None
                if img:
                    src = img.get('src') or img.get('data-src')
                    if src:
                        p = self.normalize_url(src)
                        # avoid manufacturer logo
                        if self.slug in p or 'logo' in p:
                            pass
                        else:
                            image_path = self.download_image(p)

                posts.append({'title': title, 'url': full, 'image_url': image_path, 'excerpt': ''})

            if posts:
                break

        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(posts)}")
        return posts


class ApeParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è APE Grupo"""
    
    def __init__(self):
        super().__init__('https://www.apegrupo.com/de', 'ape')
    
    def extract_logo(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ª–æ–≥–æ—Ç–∏–ø APE Grupo"""
        print(f"üîç –ü–æ–∏—Å–∫ –ª–æ–≥–æ—Ç–∏–ø–∞ –¥–ª—è {self.slug}...")
        
        soup = self.fetch_page(self.base_url)
        if not soup:
            return None
        
        # APE: –ª–æ–≥–æ—Ç–∏–ø –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É –ø—É—Ç–∏
        img = soup.find('img', src=re.compile(r'logo_apegrupo'))
        if img:
            logo_url = self.normalize_url(img.get('src'))
            if logo_url:
                return self.download_image(logo_url)
        
        print("  ‚ö†Ô∏è  –õ–æ–≥–æ—Ç–∏–ø –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ APE Grupo"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π APE Grupo...")
        
        soup = self.fetch_page(f"{self.base_url}/produkte")
        if not soup:
            return []
        
        collections = []
        
        # APE: –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ listado_buscador_productos
        container = soup.find('div', class_='listado_buscador_productos')
        if not container:
            print("  ‚ö†Ô∏è  –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        links = container.find_all('a', href=True, limit=15)
        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(links)} –∫–æ–ª–ª–µ–∫—Ü–∏–π")
        
        for idx, link in enumerate(links, 1):
            href = link.get('href', '')
            if not href or '/produkte/' not in href:
                continue
            
            print(f"  üîó –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {idx}/{len(links)}")
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ slug URL
            slug = href.rstrip('/').split('/')[-1]
            title = slug.replace('-', ' ').title()
            
            collection_url = self.normalize_url(href)
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = link.find('img')
            image_url = None
            local_image_path = None
            if img:
                image_url = self.normalize_url(img.get('src') or img.get('data-src', ''))
                if image_url:
                    local_image_path = self.download_image(image_url)
            
            if not local_image_path and not image_url:
                print(f"  ‚ö†Ô∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è {title} –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                continue
            
            collections.append({
                'title': title,
                'description': f'Serie {title}',
                'full_content': '',
                'technical_specs': '',
                'image_url': local_image_path or image_url,
                'source_url': collection_url
            })
            
            print(f"  ‚úì –ö–æ–ª–ª–µ–∫—Ü–∏—è {title} –¥–æ–±–∞–≤–ª–µ–Ω–∞")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞
            if idx < len(links):
                time.sleep(0.5)
        
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        soup = self.fetch_page(url)
        if not soup:
            return {}
        
        description = ""
        content_parts = []
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        main = soup.find(['main', 'article', 'div'], class_=lambda x: x and 'content' in str(x).lower())
        if main:
            for p in main.find_all('p', limit=5):
                text = p.get_text(strip=True)
                if len(text) > 50:
                    if not description:
                        description = text[:300]
                    content_parts.append(f"<p>{text}</p>")
        
        return {
            'description': description,
            'full_content': '\n'.join(content_parts),
            'technical_specs': ''
        }
    
    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã APE Grupo"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ APE Grupo...")
        
        soup = self.fetch_page(f"{self.base_url}/projekte")
        if not soup:
            return []
        
        projects = []
        
        # APE: –∏—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã
        project_links = soup.find_all('a', href=re.compile(r'/projekte/.+/\d+'))
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_projects = {}
        for link in project_links:
            href = link.get('href')
            if href not in unique_projects:
                unique_projects[href] = link
        
        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(unique_projects)} –ø—Ä–æ–µ–∫—Ç–æ–≤")
        
        for idx, (href, link) in enumerate(list(unique_projects.items())[:10], 1):
            print(f"  üîó –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ {idx}/{min(10, len(unique_projects))}")
            
            project_url = self.normalize_url(href)
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ
            title = link.get_text(strip=True)
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = link.find('img')
            if not img and link.parent:
                img = link.parent.find('img')
            
            image_url = None
            local_image_path = None
            if img:
                image_url = self.normalize_url(img.get('src') or img.get('data-src', ''))
                if image_url:
                    local_image_path = self.download_image(image_url)
            
            if not title or len(title) < 2:
                print(f"  ‚ö†Ô∏è  –ü—Ä–æ–µ–∫—Ç –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                continue
            
            if not local_image_path and not image_url:
                print(f"  ‚ö†Ô∏è  –ü—Ä–æ–µ–∫—Ç {title} –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                continue
            
            projects.append({
                'title': title[:100],
                'description': '',
                'full_content': '',
                'technical_specs': '',
                'image_url': local_image_path or image_url,
                'source_url': project_url
            })
            
            print(f"  ‚úì –ü—Ä–æ–µ–∫—Ç {title[:50]} –¥–æ–±–∞–≤–ª–µ–Ω")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞
            if idx < len(unique_projects):
                time.sleep(0.5)
        
        return projects
    
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ APE Grupo"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ APE Grupo...")
        
        soup = self.fetch_page(f"{self.base_url}/blog")
        if not soup:
            return []
        
        blog_posts = []
        
        # APE: –∏—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞
        blog_links = soup.find_all('a', href=re.compile(r'/blog/.+'))
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–º–∏–Ω–∏–º—É–º 4 —Å–ª–µ—à–∞)
        unique_blogs = {}
        for link in blog_links:
            href = link.get('href')
            # –¢–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
            if href.count('/') > 3 and href not in unique_blogs:
                # –ò—Å–∫–ª—é—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                if 'category' not in href:
                    unique_blogs[href] = link
        
        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(unique_blogs)} —Å—Ç–∞—Ç–µ–π –±–ª–æ–≥–∞")
        
        for idx, (href, link) in enumerate(list(unique_blogs.items())[:10], 1):
            print(f"  üîó –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ {idx}/{min(10, len(unique_blogs))}")
            
            article_url = self.normalize_url(href)
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ
            title = link.get_text(strip=True)
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = link.find('img')
            if not img and link.parent:
                img = link.parent.find('img')
            
            image_url = None
            local_image_path = None
            if img:
                image_url = self.normalize_url(img.get('src') or img.get('data-src', ''))
                if image_url:
                    local_image_path = self.download_image(image_url)
            
            if not title or len(title) < 3:
                print(f"  ‚ö†Ô∏è  –°—Ç–∞—Ç—å—è –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                continue
            
            blog_posts.append({
                'title': title[:150],
                'content': '',
                'full_content': '',
                'image_url': local_image_path or image_url,
                'source_url': article_url
            })
            
            print(f"  ‚úì –°—Ç–∞—Ç—å—è {title[:50]} –¥–æ–±–∞–≤–ª–µ–Ω–∞")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞
            if idx < len(unique_blogs):
                time.sleep(0.3)
        
        return blog_posts


class LaFabbricaParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è La Fabbrica / AVA"""
    
    def __init__(self):
        super().__init__('https://www.lafabbrica.it/de', 'lafabbrica')
    
    def extract_logo(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ª–æ–≥–æ—Ç–∏–ø La Fabbrica"""
        print(f"üîç –ü–æ–∏—Å–∫ –ª–æ–≥–æ—Ç–∏–ø–∞ –¥–ª—è {self.slug}...")
        
        soup = self.fetch_page(self.base_url)
        if not soup:
            return None
        
        # La Fabbrica: –ª–æ–≥–æ—Ç–∏–ø —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º
        img = soup.find('img', src=re.compile(r'(logo|Senza-titolo)', re.I))
        if img:
            logo_url = self.normalize_url(img.get('src'))
            if logo_url:
                return self.download_image(logo_url)
        
        print("  ‚ö†Ô∏è  –õ–æ–≥–æ—Ç–∏–ø –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ La Fabbrica –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π La Fabbrica...")
        
        collections = []
        seen_urls = set()
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–π
        categories = [
            'marmor-effekt',
            'stein-effekt',
            'holz-effekt',
            'zement-effekt',
            'metall-effekt'
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for category in categories:
            print(f"  üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
            soup = self.fetch_page(f"{self.base_url}/kollektionen/{category}/")
            if not soup:
                continue
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            # –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–∞–∂–¥–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –∏–º–µ–µ—Ç —Å—Å—ã–ª–∫—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
            links = soup.find_all('a', href=re.compile(r'/de/collections/.+'))
            
            for link in links:
                href = link.get('href')
                if not href or href in seen_urls:
                    continue
                
                # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —ç—Ç–æ–π —Å—Å—ã–ª–∫–µ –∏–ª–∏ —Ä—è–¥–æ–º —Å –Ω–µ–π
                img = link.find('img')
                if not img and link.parent:
                    # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä—è–¥–æ–º —Å —Å—Å—ã–ª–∫–æ–π
                    parent = link.parent
                    img = parent.find('img')
                
                if not img:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                img_src = img.get('src') or img.get('data-src') or img.get('data-lazy-src', '')
                if not img_src or 'wp-content' not in img_src:
                    continue
                
                # –ò—Å–∫–ª—é—á–∞–µ–º –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ GIF –∏ –ª–æ–≥–æ—Ç–∏–ø—ã
                if any(x in img_src.lower() for x in ['.gif', 'logo', 'senza-titolo', 'icon', 'menu']):
                    continue
                
                seen_urls.add(href)
                
                # –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ URL –∏–ª–∏ alt —Ç–µ–∫—Å—Ç–∞
                slug = href.rstrip('/').split('/')[-1]
                title = img.get('alt', '').strip() or slug.replace('-', ' ').title()
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ —Å–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = self.normalize_url(img_src)
                local_image_path = None
                if image_url:
                    local_image_path = self.download_image(image_url)
                
                if local_image_path or image_url:
                    collections.append({
                        'title': title,
                        'description': f'Kollektion {title}',
                        'full_content': '',
                        'technical_specs': '',
                        'image_url': local_image_path or image_url,
                        'source_url': self.normalize_url(href)
                    })
                    
                    print(f"  ‚úì –ö–æ–ª–ª–µ–∫—Ü–∏—è {title} –¥–æ–±–∞–≤–ª–µ–Ω–∞")
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π
                if len(collections) >= 15:
                    break
            
            if len(collections) >= 15:
                break
            
            time.sleep(0.3)
        
        print(f"  üìä –ò—Ç–æ–≥–æ: {len(collections)} –∫–æ–ª–ª–µ–∫—Ü–∏–π")
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        soup = self.fetch_page(url)
        if not soup:
            return {}
        
        description = ""
        content_parts = []
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        main = soup.find(['main', 'article', 'div'], class_=lambda x: x and 'content' in str(x).lower())
        if main:
            for p in main.find_all('p', limit=5):
                text = p.get_text(strip=True)
                if len(text) > 50:
                    if not description:
                        description = text[:300]
                    content_parts.append(f"<p>{text}</p>")
        
        return {
            'description': description,
            'full_content': '\n'.join(content_parts),
            'technical_specs': ''
        }
    
    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã La Fabbrica"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ La Fabbrica...")
        
        soup = self.fetch_page(f"{self.base_url}/projects")
        if not soup:
            return []
        
        projects = []
        
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –ø—Ä–æ–µ–∫—Ç–∞–º–∏
        containers = soup.find_all(['div', 'article'], class_=lambda x: x and 'post' in str(x).lower())
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–µ–∫—Ç—ã
        project_links = soup.find_all('a', href=re.compile(r'/de/projects/.+'))
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_projects = {}
        for link in project_links:
            href = link.get('href')
            # –¢–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã–µ URL –ø—Ä–æ–µ–∫—Ç–æ–≤ (–¥–ª–∏–Ω–Ω–µ–µ 40 —Å–∏–º–≤–æ–ª–æ–≤)
            if len(href) > 40 and href not in unique_projects:
                unique_projects[href] = link
        
        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(unique_projects)} –ø—Ä–æ–µ–∫—Ç–æ–≤")
        
        for idx, (href, link) in enumerate(list(unique_projects.items())[:10], 1):
            print(f"  üîó –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ {idx}/{min(10, len(unique_projects))}")
            
            project_url = self.normalize_url(href)
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏ –∏–ª–∏ –∏–∑ URL
            title = link.get_text(strip=True)
            if not title or len(title) < 3:
                # –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ URL
                slug = href.rstrip('/').split('/')[-1]
                title = slug.replace('-', ' ').title()
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = link.find('img')
            if not img and link.parent:
                img = link.parent.find('img')
            
            image_url = None
            local_image_path = None
            if img:
                image_url = self.normalize_url(img.get('src') or img.get('data-src') or img.get('data-lazy-src', ''))
                if image_url and 'placeholder' not in image_url:
                    local_image_path = self.download_image(image_url)
            
            if not title or len(title) < 3:
                print(f"  ‚ö†Ô∏è  –ü—Ä–æ–µ–∫—Ç –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                continue
            
            if not local_image_path and not image_url:
                print(f"  ‚ö†Ô∏è  –ü—Ä–æ–µ–∫—Ç {title} –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                continue
            
            projects.append({
                'title': title[:100],
                'description': '',
                'full_content': '',
                'technical_specs': '',
                'image_url': local_image_path or image_url,
                'source_url': project_url
            })
            
            print(f"  ‚úì –ü—Ä–æ–µ–∫—Ç {title[:50]} –¥–æ–±–∞–≤–ª–µ–Ω")
            
            time.sleep(0.5)
        
        return projects
    
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ La Fabbrica"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ La Fabbrica...")
        
        soup = self.fetch_page(f"{self.base_url}/blog")
        if not soup:
            return []
        
        blog_posts = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
        blog_links = soup.find_all('a', href=re.compile(r'/de/blog/.+'))
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        unique_blogs = {}
        for link in blog_links:
            href = link.get('href')
            # –¢–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ (–¥–ª–∏–Ω–Ω—ã–µ URL)
            if len(href) > 40 and 'category' not in href and href not in unique_blogs:
                unique_blogs[href] = link
        
        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(unique_blogs)} —Å—Ç–∞—Ç–µ–π –±–ª–æ–≥–∞")
        
        for idx, (href, link) in enumerate(list(unique_blogs.items())[:10], 1):
            print(f"  üîó –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ {idx}/{min(10, len(unique_blogs))}")
            
            article_url = self.normalize_url(href)
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ
            title = link.get_text(strip=True)
            if not title or len(title) < 3:
                slug = href.rstrip('/').split('/')[-1]
                title = slug.replace('-', ' ').title()
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = link.find('img')
            if not img and link.parent:
                img = link.parent.find('img')
            
            image_url = None
            local_image_path = None
            if img:
                image_url = self.normalize_url(img.get('src') or img.get('data-src') or img.get('data-lazy-src', ''))
                if image_url:
                    local_image_path = self.download_image(image_url)
            
            if not title or len(title) < 3:
                print(f"  ‚ö†Ô∏è  –°—Ç–∞—Ç—å—è –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                continue
            
            blog_posts.append({
                'title': title[:150],
                'content': '',
                'full_content': '',
                'image_url': local_image_path or image_url,
                'source_url': article_url
            })
            
            print(f"  ‚úì –°—Ç–∞—Ç—å—è {title[:50]} –¥–æ–±–∞–≤–ª–µ–Ω–∞")
            
            time.sleep(0.3)
        
        return blog_posts


class BaldocerParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Baldocer"""
    
    def __init__(self):
        super().__init__('https://baldocer.com', 'baldocer')
    
    def extract_logo(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ª–æ–≥–æ—Ç–∏–ø Baldocer"""
        print(f"üîç –ü–æ–∏—Å–∫ –ª–æ–≥–æ—Ç–∏–ø–∞ –¥–ª—è {self.slug}...")
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–π URL –ª–æ–≥–æ—Ç–∏–ø–∞
        logo_url = f"{self.base_url}/wp-content/uploads/2018/06/logo.png"
        return self.download_image(logo_url)
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Baldocer (–ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ –ª–∏–Ω–µ–π–∫–∏)"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Baldocer...")
        
        collections = []
        
        soup = self.fetch_page(f"{self.base_url}/producto/")
        if not soup:
            return collections
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ Baldocer
        categories = [
            ('porcelanico', 'Pasta porcel√°nica'),
            ('pasta-blanca', 'Pasta blanca'),
            ('bplus', 'b|plus'),
            ('bthin', 'b|thin'),
            ('bout', 'b|&out')
        ]
        
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ /producto/
        category_images = {}
        for img in soup.find_all('img'):
            parent_link = img.find_parent('a')
            if not parent_link:
                continue
            
            href = parent_link.get('href', '')
            src = img.get('src') or img.get('data-src')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–µ–¥–µ—Ç –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ –æ–¥–Ω—É –∏–∑ –Ω–∞—à–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            for slug, name in categories:
                if f'/producto/{slug}/' in href and src and '/uploads/' in src:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–µ–ª–∫–∏–µ –∏–∫–æ–Ω–∫–∏
                    if any(skip in src.lower() for skip in ['logo', 'icon', 'flag', 'facebook', 'instagram', 'linkedin', 'pinterest', 'youtube', 'login']):
                        continue
                    
                    category_images[slug] = src
                    break
        
        print(f"  üì∏ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(category_images)}")
        
        # –¢–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        for slug, name in categories:
            category_url = f"{self.base_url}/producto/{slug}/"
            
            print(f"  üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {name}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            image_url = None
            if slug in category_images:
                full_url = self.normalize_url(category_images[slug])
                image_url = self.download_image(full_url)
            
            collection_data = {
                'title': name,
                'url': category_url,
                'image_url': image_url,
                'description': f'Baldocer {name}'
            }
            
            collections.append(collection_data)
            print(f"    ‚úÖ {name}: {'–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ' if image_url else '–±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è'}")
            
            time.sleep(0.2)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(collections)} –∫–æ–ª–ª–µ–∫—Ü–∏–π")
        return collections
    
    def extract_collection_detail(self, collection_url: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Baldocer"""
        print(f"  üîé –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_url}")
        
        soup = self.fetch_page(collection_url)
        if not soup:
            return None
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        title_tag = soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else collection_url.rstrip('/').split('/')[-1]
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and '/uploads/' in src:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–æ–≥–æ—Ç–∏–ø—ã, –∏–∫–æ–Ω–∫–∏
                if any(skip in src.lower() for skip in ['logo', 'icon', 'flag', 'facebook', 'instagram', 'linkedin', 'pinterest', 'youtube']):
                    continue
                
                full_url = self.normalize_url(src)
                image_path = self.download_image(full_url)
                if image_path:
                    images.append(image_path)
                    if len(images) >= 5:
                        break
        
        # –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
        description = ""
        meta_desc = soup.find('meta', {'name': 'description'}) or soup.find('meta', {'property': 'og:description'})
        if meta_desc:
            description = meta_desc.get('content', '')
        
        return {
            'title': title,
            'url': collection_url,
            'description': description,
            'images': images
        }
    
    def extract_projects(self) -> List[Dict]:
        """Baldocer –Ω–µ –∏–º–µ–µ—Ç —Ä–∞–∑–¥–µ–ª–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤"""
        print("  ‚ÑπÔ∏è  Baldocer –Ω–µ –∏–º–µ–µ—Ç —Ä–∞–∑–¥–µ–ª–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤")
        return []
    
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ Baldocer"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ Baldocer...")
        
        blog_posts = []
        
        soup = self.fetch_page(f"{self.base_url}/noticias/")
        if not soup:
            return blog_posts
        
        # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏
        articles = soup.find_all('article') or soup.find_all('div', class_=re.compile(r'post|article|news', re.I))
        
        for article in articles[:10]:
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é
            link = article.find('a', href=True)
            if not link:
                continue
            
            href = link.get('href')
            if not href or '/noticias/' not in href:
                continue
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ
            title_tag = article.find(['h1', 'h2', 'h3', 'h4'])
            title = title_tag.get_text(strip=True) if title_tag else link.get_text(strip=True)
            
            if not title or len(title) < 3:
                continue
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = article.find('img')
            image_url = None
            if img:
                src = img.get('src') or img.get('data-src')
                if src:
                    full_url = self.normalize_url(src)
                    image_url = self.download_image(full_url)
            
            blog_posts.append({
                'title': title,
                'url': self.normalize_url(href),
                'image_url': image_url,
                'excerpt': ''
            })
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(blog_posts)} —Å—Ç–∞—Ç–µ–π")
        return blog_posts


class CasalgrandeParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Casalgrande Padana"""

    def __init__(self):
        super().__init__('https://www.casalgrandepadana.com', 'casalgrande')

    def extract_logo(self) -> Optional[str]:
        """–ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ª–æ–≥–æ—Ç–∏–ø –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        print(f"üîç –ü–æ–∏—Å–∫ –ª–æ–≥–æ—Ç–∏–ø–∞ –¥–ª—è {self.slug}...")
        soup = self.fetch_page(self.base_url)
        if not soup:
            return None
        # 1) –ü–æ–∏—Å–∫ –ø–æ <img> —Å alt/class/src —Å–æ–¥–µ—Ä–∂–∞—â–∏–º 'logo'
        img = soup.find('img', {'alt': re.compile(r'logo', re.I)}) or soup.find('img', class_=re.compile(r'logo', re.I)) or soup.find('img', src=re.compile(r'logo', re.I))
        if img:
            src = img.get('src') or img.get('data-src') or img.get('data-lazy')
            if src and not src.startswith('data:'):
                return self.download_image(self.normalize_url(src))

        # 2) Meta og:image / twitter:image
        meta_og = soup.find('meta', property=re.compile(r'og:image', re.I)) or soup.find('meta', attrs={'name': re.compile(r'twitter:image', re.I)})
        if meta_og and meta_og.get('content'):
            og = meta_og.get('content')
            if og and not og.startswith('data:'):
                return self.download_image(self.normalize_url(og))

        # 3) link rel icons (favicon / apple-touch-icon)
        link_icon = soup.find('link', rel=re.compile(r'icon', re.I))
        if link_icon and link_icon.get('href'):
            href = link_icon.get('href')
            if href and not href.startswith('data:'):
                path = self.download_image(self.normalize_url(href))
                if path:
                    return path

        # 4) –ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å inline <svg> –∫–∞–∫ —Ñ–∞–π–ª
        svg = soup.find('svg', class_=re.compile(r'logo', re.I)) or soup.find('svg', id=re.compile(r'logo', re.I))
        if svg:
            try:
                svg_str = str(svg)
                # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ svg –≤ uploads
                url_hash = hashlib.md5(svg_str.encode()).hexdigest()[:10]
                filename = f"{self.slug}_logo_{url_hash}.svg"
                upload_dir = os.path.join('app', 'static', 'uploads', 'manufacturers')
                os.makedirs(upload_dir, exist_ok=True)
                filepath = os.path.join(upload_dir, filename)
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(svg_str)
                print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω—ë–Ω inline SVG –∫–∞–∫ {filename}")
                return f'manufacturers/{filename}'
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è inline SVG: {e}")

        # 5) –®–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫: –ø–µ—Ä–≤—ã–π –ª–æ–≥–æ—Ç–∏–ø-–ø–æ—Ö–æ–∂–∏–π <img> –≤ header/nav –∏–ª–∏ —Å –Ω–µ–±–æ–ª—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º
        header = soup.find(['header', 'nav'])
        candidates = []
        if header:
            candidates = header.find_all('img')
        if not candidates:
            candidates = soup.find_all('img', src=True)[:20]

        for img in candidates:
            src = img.get('src') or img.get('data-src') or img.get('data-lazy')
            if not src or src.startswith('data:'):
                continue
            # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (media —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –ø—É—Ç—è–º–∏), –Ω–æ –ø–æ–∑–≤–æ–ª–∏–º –ª–æ–≥–æ—Ç–∏–ø—ã
            lowered = src.lower()
            if 'logo' in lowered or 'favicon' in lowered or 'brand' in lowered or 'casalgrandepadana' in lowered:
                path = self.download_image(self.normalize_url(src))
                if path:
                    return path

        print("  ‚ö†Ô∏è  –õ–æ–≥–æ—Ç–∏–ø –Ω–µ –Ω–∞–π–¥–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
        return None

    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏/–ø—Ä–æ–¥—É–∫—Ç—ã —á–µ—Ä–µ–∑ sitemap-prodotti.xml"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Casalgrande —á–µ—Ä–µ–∑ sitemap...")
        collections = []

        sitemap_url = f"{self.base_url}/sitemap-prodotti.xml"
        try:
            r = requests.get(sitemap_url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            if r.status_code != 200:
                print(f"  ‚ö†Ô∏è  Sitemap –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {sitemap_url} -> {r.status_code}")
                return collections

            soup = BeautifulSoup(r.content, 'xml')
            urls = soup.find_all('url')
            print(f"  –í sitemap –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(urls)}")

            for url_tag in urls:
                loc = url_tag.find('loc')
                if not loc:
                    continue
                page_url = loc.get_text(strip=True)

                # –¢–æ–ª—å–∫–æ product pages (/product/slug)
                if '/product/' not in page_url:
                    continue

                title = page_url.rstrip('/').split('/')[-1].replace('-', ' ').title()
                image_tag = url_tag.find('image:loc') or url_tag.find('image')
                image_url = None
                if image_tag:
                    img = image_tag.get_text(strip=True)
                    if img:
                        image_url = self.download_image(self.normalize_url(img))

                collections.append({
                    'title': title,
                    'url': page_url,
                    'image_url': image_url,
                    'description': ''
                })

                if len(collections) >= 200:
                    break

        except Exception as e:
            print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ sitemap: {e}")

        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")
        return collections

    def extract_collection_detail(self, collection_url: str) -> Optional[Dict]:
        print(f"  üîé –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_url}")
        soup = self.fetch_page(collection_url)
        if not soup:
            return None

        title_tag = soup.find(['h1', 'h2'])
        title = title_tag.get_text(strip=True) if title_tag else collection_url.rstrip('/').split('/')[-1]

        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and 'media' in src or 'filer_public' in src:
                path = self.download_image(self.normalize_url(src))
                if path:
                    images.append(path)
            if len(images) >= 8:
                break

        desc = ''
        meta = soup.find('meta', {'name': 'description'}) or soup.find('meta', {'property': 'og:description'})
        if meta:
            desc = meta.get('content', '')

        return {
            'title': title,
            'url': collection_url,
            'description': desc,
            'images': images
        }

    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã —á–µ—Ä–µ–∑ sitemap-realizzazioni.xml"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ Casalgrande —á–µ—Ä–µ–∑ sitemap...")
        projects = []
        sitemap = f"{self.base_url}/sitemap-realizzazioni.xml"
        try:
            r = requests.get(sitemap, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            if r.status_code != 200:
                return projects
            soup = BeautifulSoup(r.content, 'xml')
            for url_tag in soup.find_all('url')[:100]:
                loc = url_tag.find('loc')
                if not loc:
                    continue
                page_url = loc.get_text(strip=True)
                title = page_url.rstrip('/').split('/')[-1].replace('-', ' ').title()
                # try to find image
                image_tag = url_tag.find('image:loc') or url_tag.find('image')
                image = None
                if image_tag:
                    image = self.download_image(self.normalize_url(image_tag.get_text(strip=True)))
                projects.append({'title': title, 'url': page_url, 'image_url': image, 'description': ''})
        except Exception as e:
            print('  Error reading projects sitemap:', e)
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–µ–∫—Ç–æ–≤: {len(projects)}")
        return projects

    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ sitemap-news.xml"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π Casalgrande —á–µ—Ä–µ–∑ sitemap...")
        posts = []
        sitemap = f"{self.base_url}/sitemap-news.xml"
        try:
            r = requests.get(sitemap, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            if r.status_code != 200:
                return posts
            soup = BeautifulSoup(r.content, 'xml')
            for url_tag in soup.find_all('url')[:100]:
                loc = url_tag.find('loc')
                if not loc:
                    continue
                page_url = loc.get_text(strip=True)
                title = page_url.rstrip('/').split('/')[-1].replace('-', ' ').title()
                posts.append({'title': title, 'url': page_url, 'image_url': None, 'excerpt': ''})
        except Exception as e:
            print('  Error reading news sitemap:', e)
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(posts)}")
        return posts


class DistrimatParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Distrimat (https://www.distrimat.es/en)"""

    def __init__(self):
        super().__init__('https://www.distrimat.es/en', 'distrimat')

    def extract_logo(self) -> Optional[str]:
        print(f"üîç –ü–æ–∏—Å–∫ –ª–æ–≥–æ—Ç–∏–ø–∞ –¥–ª—è {self.slug}...")
        soup = self.fetch_page(self.base_url)
        if not soup:
            return None

        # –ò—â–µ–º —è–≤–Ω—ã–π –ª–æ–≥–æ—Ç–∏–ø –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–ª–∏ –ø–æ alt
        img = soup.find('img', alt=re.compile(r'logotipo|logo', re.I))
        if not img:
            img = soup.find('img', src=re.compile(r'distrimat', re.I))
        if img:
            src = img.get('src') or img.get('data-src')
            if src and not src.startswith('data:'):
                return self.download_image(self.normalize_url(src))

        # –§–æ–ª–ª–±–µ–∫: og:image
        meta = soup.find('meta', property=re.compile(r'og:image', re.I))
        if meta and meta.get('content'):
            return self.download_image(self.normalize_url(meta.get('content')))

        print('  ‚ö†Ô∏è  –õ–æ–≥–æ—Ç–∏–ø –Ω–µ –Ω–∞–π–¥–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏')
        return None

    def extract_collections(self) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–µ—Ä–∏–π/–∫–æ–ª–ª–µ–∫—Ü–∏–π —á–µ—Ä–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Ceramics"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Distrimat...")
        collections = []

        category_path = '/categoria-producto/ceramics-en/'
        soup = self.fetch_page(f"{self.base_url}{category_path}")
        if not soup:
            return collections

        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏/—Å–µ—Ä–∏–∏
        links = []
        for a in soup.find_all('a', href=True):
            href = a.get('href')
            if href and '/categoria-producto/' in href and href.rstrip('/') != f"{self.base_url}{category_path.rstrip('/')}":
                links.append(a)

        # –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ href
        seen = set()
        filtered = []
        for a in links:
            href = a.get('href')
            if href and href not in seen:
                seen.add(href)
                filtered.append(a)

        print(f"  –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ä–∏–π: {len(filtered)}")

        for idx, a in enumerate(filtered[:200], 1):
            href = a.get('href')
            title = a.get_text(strip=True) or href.rstrip('/').split('/')[-1].replace('-', ' ').title()
            image_url = None
            img = a.find('img')
            if img:
                src = img.get('src') or img.get('data-src')
                if src:
                    image_url = self.download_image(self.normalize_url(src))

            collections.append({'title': title, 'description': '', 'image_url': image_url, 'source_url': href})

            if idx < len(filtered[:200]):
                time.sleep(0.2)

        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")
        return collections

    def extract_collection_detail(self, url: str) -> Optional[Dict]:
        print(f"  üîé –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {url}")
        soup = self.fetch_page(url)
        if not soup:
            return None

        title_tag = soup.find(['h1', 'h2'])
        title = title_tag.get_text(strip=True) if title_tag else url.rstrip('/').split('/')[-1]

        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and ('wp-content' in src or 'uploads' in src):
                p = self.download_image(self.normalize_url(src))
                if p:
                    images.append(p)
            if len(images) >= 8:
                break

        desc = ''
        meta = soup.find('meta', {'name': 'description'}) or soup.find('meta', {'property': 'og:description'})
        if meta:
            desc = meta.get('content', '')

        return {'title': title, 'url': url, 'description': desc, 'images': images}

    def extract_projects(self) -> List[Dict]:
        return []

    def extract_blog_posts(self) -> List[Dict]:
        return []


class EstudiCeremicoParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Estudi Ceramico"""
    
    def __init__(self):
        super().__init__('https://eceramico.com', 'estudi-ceramico')
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Estudi Ceramico"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Estudi Ceramico...")
        
        soup = self.fetch_page(f"{self.base_url}/en/collections/")
        if not soup:
            return []
        
        collections = []
        seen = set()
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–µ—Ä–∏–∏ (formato: /serie/{slug}/)
        links = soup.find_all('a', href=re.compile(r'/serie/'))
        
        for link in links:
            href = link.get('href')
            if not href:
                continue
            
            full_url = self.normalize_url(href)
            if full_url in seen:
                continue
            seen.add(full_url)
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = link.get_text(strip=True)
            if not title or len(title) < 2:
                continue
            
            print(f"  üîó –ö–æ–ª–ª–µ–∫—Ü–∏—è: {title}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            detail = self.extract_collection_detail(full_url) or {}
            
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ—Ç–∞–ª–µ–π
            chosen_image = None
            detail_images = detail.get('images') or []
            if detail_images:
                chosen_image = detail_images[0]
            
            collections.append({
                'title': title,
                'description': detail.get('description', ''),
                'full_content': detail.get('full_content', ''),
                'technical_specs': detail.get('technical_specs', ''),
                'image_url': chosen_image,
                'source_url': full_url
            })
            
            time.sleep(0.2)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Estudi Ceramico"""
        soup = self.fetch_page(url)
        if not soup:
            return {}
        
        description = ""
        content_parts = []
        images = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        h1 = soup.find('h1')
        if h1:
            title = h1.get_text(strip=True)
        else:
            title = url.rstrip('/').split('/')[-1].title()
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        desc_elem = soup.find('div', class_=lambda x: x and any(k in str(x).lower() for k in ['content', 'description', 'intro']))
        if desc_elem:
            description = desc_elem.get_text(strip=True)[:300]
        
        # –°–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∏—Å–∫–ª—é—á–∞–µ–º –ª–æ–≥–æ—Ç–∏–ø—ã –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é)
        candidates = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if not src or src.startswith('data:'):
                continue
            
            norm = self.normalize_url(src)
            low = norm.lower()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–æ–≥–æ—Ç–∏–ø—ã, —Ñ–ª–∞–≥–∏ –∏ –∏–∫–æ–Ω–∫–∏
            if any(k in low for k in ['logo', 'flag', 'icon', 'svg', 'loader']):
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ –∏–∫–æ–Ω–∫–∏)
            if '/res/' in low or '/flags/' in low or '/assets/' in low:
                continue
            
            candidates.append(norm)
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        seen = set()
        unique = []
        for c in candidates:
            if c not in seen:
                seen.add(c)
                unique.append(c)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ 8 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        for src in unique:
            p = self.download_image(src)
            if p:
                images.append(p)
            if len(images) >= 8:
                break
        
        return {
            'title': title,
            'description': description,
            'full_content': '\n'.join(content_parts),
            'technical_specs': '',
            'images': images
        }
    
    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã Estudi Ceramico"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ Estudi Ceramico...")
        
        soup = self.fetch_page(f"{self.base_url}/en/projects/")
        if not soup:
            return []
        
        projects = []
        seen = set()
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–µ–∫—Ç—ã –≤ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        links = soup.find_all('a', href=lambda x: x and '/projects/' in x and x != f"{self.base_url}/en/projects/")
        
        for link in links:
            href = link.get('href')
            if not href:
                continue
            
            full_url = self.normalize_url(href)
            if full_url in seen:
                continue
            seen.add(full_url)
            
            title = link.get_text(strip=True)
            if not title or len(title) < 2:
                continue
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä—è–¥–æ–º —Å —Å—Å—ã–ª–∫–æ–π
            img = link.find('img')
            if not img:
                parent = link.find_parent(['div', 'article', 'figure'])
                if parent:
                    img = parent.find('img')
            
            image = None
            if img:
                src = img.get('src') or img.get('data-src')
                if src and not src.startswith('data:'):
                    image = self.download_image(self.normalize_url(src))
            
            if not image:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–µ–∫—Ç—ã –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            
            projects.append({
                'title': title,
                'description': '',
                'full_content': '',
                'image_url': image,
                'source_url': full_url
            })
            
            time.sleep(0.2)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–µ–∫—Ç–æ–≤: {len(projects)}")
        return projects
    
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ Estudi Ceramico"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ Estudi Ceramico...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ URL (—Å /en/ –∏ –±–µ–∑)
        candidates = [f"{self.base_url}/en/blog/", f"{self.base_url}/blog/"]
        soup = None
        blog_url = None
        
        for c in candidates:
            soup = self.fetch_page(c)
            if soup:
                blog_url = c
                break
        
        if not soup:
            return []
        
        posts = []
        seen = set()
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞
        links = soup.find_all('a', href=lambda x: x and '/blog/' in x)
        
        for link in links:
            href = link.get('href')
            if not href or href == blog_url:
                continue
            
            full_url = self.normalize_url(href)
            if full_url in seen:
                continue
            seen.add(full_url)
            
            title = link.get_text(strip=True)
            if not title or len(title) < 3:
                continue
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            img = link.find('img')
            if not img:
                parent = link.find_parent(['div', 'article', 'figure'])
                if parent:
                    img = parent.find('img')
            
            image = None
            if img:
                src = img.get('src') or img.get('data-src')
                if src and not src.startswith('data:') and 'logo' not in src.lower():
                    image = self.download_image(self.normalize_url(src))
            
            # –î–ª—è –±–ª–æ–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
            posts.append({
                'title': title,
                'excerpt': '',
                'full_content': '',
                'image_url': image,
                'url': full_url
            })
            
            time.sleep(0.1)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(posts)}")
        return posts

class EtileParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Etile Ceramics"""
    
    def __init__(self):
        super().__init__('https://en.etile.es', 'etile')
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Etile Ceramics"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Etile Ceramics...")
        
        soup = self.fetch_page(f"{self.base_url}/etile/")
        if not soup:
            return []
        
        collections = []
        
        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –∫–æ–ª–ª–µ–∫—Ü–∏–π (–∏—Å–ø–æ–ª—å–∑—É—é—Ç jQuery gridder)
        gridder_items = soup.find_all('li', class_='gridder-list')
        
        for item in gridder_items:
            # –ü–æ–ª—É—á–∞–µ–º slug –∏–∑ id
            item_id = item.get('id', '')
            if not item_id.startswith('collection-'):
                continue
            
            slug = item_id.replace('collection-', '')
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            content_ref = item.get('data-griddercontent', '')
            if not content_ref:
                continue
            
            print(f"  üîó –ö–æ–ª–ª–µ–∫—Ü–∏—è: {slug}")
            
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ä–∞–∑–¥–µ–ª —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            content_id = content_ref.lstrip('#')
            content_elem = soup.find(id=content_id)
            
            images = []
            description = ""
            
            if content_elem:
                # –ò—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤–æ–µ, –∫–æ—Ç–æ—Ä–æ–µ –±–æ–ª—å—à–µ 5KB
                all_imgs = content_elem.find_all('img')
                
                for img in all_imgs:
                    img_src = img.get('src', '')
                    img_alt = img.get('alt', '')
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∫–æ–Ω–∫–∏ (–æ–±—ã—á–Ω–æ < 1KB)
                    if not img_src or 'icon' in img_alt.lower() or 'logo' in img_alt.lower():
                        continue
                    
                    img_path = self.download_image(img_src)
                    if img_path:
                        images.append(img_path)
                        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø–µ—Ä–≤–æ–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ)
                        break
                
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ
                text_elem = content_elem.find('p') or content_elem.find('div', class_='description')
                if text_elem:
                    description = text_elem.get_text(strip=True)[:300]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤—Å–µ —Ä–∞–≤–Ω–æ, –¥–∞–∂–µ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            collections.append({
                'title': slug.replace('-', ' ').title(),
                'description': description,
                'full_content': '',
                'technical_specs': '',
                'image_url': images[0] if images else None,
                'source_url': f"{self.base_url}/etile/#{slug}"
            })
            
            time.sleep(0.1)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (–¥–ª—è Etile –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è)"""
        return {}
    
    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã Etile Ceramics"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ Etile Ceramics...")
        # –ù–∞ —Å–∞–π—Ç–µ Etile –Ω–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–µ–∫—Ç–æ–≤
        print("  ‚ÑπÔ∏è  –ü—Ä–æ–µ–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return []
    
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ Etile Ceramics"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ Etile Ceramics...")
        # –ù–∞ —Å–∞–π—Ç–µ Etile –Ω–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –±–ª–æ–≥–∞
        print("  ‚ÑπÔ∏è  –ë–ª–æ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []


class ExagresParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Exagres Ceramics"""
    
    def __init__(self):
        super().__init__('https://www.exagres.es', 'exagres')
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Exagres Ceramics"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Exagres Ceramics...")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        soup = self.fetch_page(f"{self.base_url}/colecciones-residencial/")
        if not soup:
            return []
        
        collections = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        all_links = soup.find_all('a', href=True)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∫–æ–ª–ª–µ–∫—Ü–∏–π
        collection_keywords = ['gresan', 'pavim', 'piscina', 'torelo', 'fachada', 'suelo', 'vierteagua', 'pasamano', 'deck']
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            if not href or len(text) < 3:
                continue
            
            if not any(keyword in href.lower() for keyword in collection_keywords):
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            if any(c['source_url'] == href for c in collections):
                continue
            
            print(f"  üîó –ö–æ–ª–ª–µ–∫—Ü–∏—è: {text}")
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            image_url = None
            description = ""
            
            # –ü–æ—Å–µ—â–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_soup = self.fetch_page(href)
            if collection_soup:
                # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ main –∫–æ–Ω—Ç–µ–Ω—Ç–µ (–Ω–µ –≤ header/footer)
                # –ò—â–µ–º –≤ –±–æ–ª—å—à–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö, –≥–¥–µ –æ–±—ã—á–Ω–æ –ª–µ–∂–∏—Ç –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                
                image_candidates = []
                
                # 1. –ò—â–µ–º –≤ <picture>, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                picture_tags = collection_soup.find_all('picture')
                for picture in picture_tags:
                    img = picture.find('img')
                    if img:
                        src = img.get('src', '')
                        if src and 'icon' not in src.lower() and 'logo' not in src.lower():
                            image_candidates.append(src)
                
                # 2. –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö (hero, main, content)
                main_containers = collection_soup.find_all(['div', 'section'], class_=lambda x: x and any(
                    keyword in str(x).lower() for keyword in ['hero', 'main', 'content', 'featured', 'collection', 'banner']
                ))
                
                for container in main_containers:
                    imgs = container.find_all('img')
                    for img in imgs:
                        src = img.get('src', '')
                        alt = img.get('alt', '')
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏, –ª–æ–≥–æ—Ç–∏–ø—ã, –∏ —è–≤–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if not src or 'icon' in alt.lower() or 'logo' in alt.lower():
                            continue
                        
                        # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –∫–æ–Ω—Ç–µ–Ω—Ç (–ø–æ alt —Ç–µ–∫—Å—Ç—É)
                        if alt and len(alt) > 3:
                            image_candidates.insert(0, src)  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                        else:
                            image_candidates.append(src)
                
                # 3. –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤—Å–µ img –∫—Ä–æ–º–µ —è–≤–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö
                if not image_candidates:
                    img_tags = collection_soup.find_all('img')
                    for img in img_tags:
                        src = img.get('src', '')
                        alt = img.get('alt', '')
                        width = img.get('width', '')
                        height = img.get('height', '')
                        
                        if not src or 'icon' in alt.lower() or 'logo' in alt.lower():
                            continue
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∏–∫–æ–Ω–∫–∏)
                        if width and int(str(width).replace('px', '')) < 200:
                            continue
                        if height and int(str(height).replace('px', '')) < 200:
                            continue
                        
                        image_candidates.append(src)
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –ª—É—á—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç
                for candidate in image_candidates:
                    if not candidate:
                        continue
                    img_path = self.download_image(candidate)
                    if img_path:
                        image_url = img_path
                        break
                
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ
                desc_elem = collection_soup.find('p') or collection_soup.find('div', class_=lambda x: x and 'description' in x.lower())
                if desc_elem:
                    description = desc_elem.get_text(strip=True)[:300]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            collections.append({
                'title': text,
                'description': description,
                'full_content': '',
                'technical_specs': '',
                'image_url': image_url,
                'source_url': href
            })
            
            time.sleep(0.2)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        return {}
    
    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã Exagres Ceramics"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ Exagres Ceramics...")
        # –ù–∞ —Å–∞–π—Ç–µ Exagres –Ω–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–µ–∫—Ç–æ–≤
        print("  ‚ÑπÔ∏è  –ü—Ä–æ–µ–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return []
    
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ Exagres Ceramics"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ Exagres Ceramics...")
        
        blog_posts = []
        soup = self.fetch_page(f"{self.base_url}/blog/")
        if not soup:
            print("  ‚ÑπÔ∏è  –ë–ª–æ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞
        blog_links = soup.find_all('a', href=lambda x: x and '/blog/' in x and len(x) > 10)
        
        for link in blog_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é
            if not href or len(text) < 5 or href.endswith('/blog/') or 'http' not in href and self.base_url not in href:
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ
            if any(p['source_url'] == href for p in blog_posts):
                continue
            
            print(f"  üìù –°—Ç–∞—Ç—å—è: {text[:60]}")
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
            description = ""
            images = []
            
            post_soup = self.fetch_page(href)
            if post_soup:
                # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (–ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü)
                p_tag = post_soup.find('p')
                if p_tag:
                    description = p_tag.get_text(strip=True)[:300]
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                img = post_soup.find('img')
                if img:
                    src = img.get('src', '')
                    img_path = self.download_image(src)
                    if img_path:
                        images.append(img_path)
            
            blog_posts.append({
                'title': text,
                'description': description,
                'full_content': '',
                'image_url': images[0] if images else None,
                'source_url': href,
                'published': True
            })
            
            time.sleep(0.1)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(blog_posts)}")
        return blog_posts


class HalconParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Halcon Ceramicas"""
    
    def __init__(self):
        super().__init__('https://www.halconceramicas.com', 'halcon')
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Halcon Ceramicas"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Halcon Ceramicas...")
        
        soup = self.fetch_page(f"{self.base_url}/colecciones")
        if not soup:
            return []
        
        collections = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        for link in soup.find_all('a', href=lambda x: x and '/colecciones/' in x):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            if not href or len(text) < 2:
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
            if not href.startswith('http'):
                href = urljoin(self.base_url, href)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ
            if any(c['source_url'] == href for c in collections):
                continue
            
            print(f"  üîó –ö–æ–ª–ª–µ–∫—Ü–∏—è: {text}")
            
            images = []
            description = ""
            
            # –ü–æ—Å–µ—â–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_soup = self.fetch_page(href)
            if collection_soup:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏–∑ URL –¥–ª—è –ø–æ–∏—Å–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                # –ù–∞–ø—Ä–∏–º–µ—Ä: "coleccion-capri" -> "capri"
                url_parts = href.rstrip('/').split('/')
                collection_slug = url_parts[-1] if url_parts else text.lower()
                
                # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã —Ç–∏–ø–∞ "coleccion-", "dolomite-" –∏ —Ç.–¥.
                if '-' in collection_slug:
                    # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–Ω–∞—á–∞—â—É—é —á–∞—Å—Ç—å –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è "coleccion-"
                    parts = collection_slug.replace('coleccion-', '').split('-')
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ 2 —á–∞—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä "coleccion-grand-canyon" -> "grand-canyon")
                    collection_slug = '-'.join(parts[:2]) if len(parts) > 1 else parts[0]
                
                # –ò—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                img_tags = collection_soup.find_all('img')
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                collection_images = []  # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                fallback_images = []    # –î—Ä—É–≥–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ /storage/
                
                for img in img_tags:
                    src = img.get('src', '')
                    
                    if not src:
                        continue
                    
                    src_lower = src.lower()
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω—ã–µ –ª–æ–≥–æ—Ç–∏–ø—ã, –∏–∫–æ–Ω–∫–∏ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ–¥–∏–∞
                    if any(word in src_lower for word in 
                           ['logo', 'icon', 'instagram', 'facebook', 'twitter', 'linkedin', 
                            'pixel', 'tracking', 'svg', '1x1', 'white-pixel']):
                        continue
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
                    if '/storage/' in src_lower:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (thumbnails –∏ PNG)
                        if 'thumb' not in src_lower and '.png' not in src_lower:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                            if collection_slug in src_lower:
                                collection_images.append(src)
                            else:
                                fallback_images.append(src)
                
                # –í—ã–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                img_to_download = None
                
                # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º medium –∫–∞—á–µ—Å—Ç–≤–æ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                for img_src in collection_images:
                    if 'medium' in img_src.lower():
                        img_to_download = img_src
                        break
                
                # –ï—Å–ª–∏ –Ω–µ—Ç medium, –∏—â–µ–º –ª—é–±–æ–µ –¥—Ä—É–≥–æ–µ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                if not img_to_download and collection_images:
                    img_to_download = collection_images[0]
                
                # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback (–Ω–æ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å)
                # if not img_to_download and fallback_images:
                #     img_to_download = fallback_images[0]
                
                if img_to_download:
                    img_path = self.download_image(img_to_download)
                    if img_path:
                        images.append(img_path)
                
                # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                paras = collection_soup.find_all('p')
                for p in paras:
                    text_content = p.get_text(strip=True)
                    if len(text_content) > 20 and 'javascript' not in text_content.lower():
                        description = text_content[:300]
                        break
            
            collections.append({
                'title': text,
                'description': description,
                'full_content': '',
                'technical_specs': '',
                'image_url': images[0] if images else None,
                'source_url': href
            })
            
            time.sleep(0.15)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        return {}
    
    def extract_projects(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã Halcon Ceramicas"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤ Halcon Ceramicas...")
        
        # –ù–∞ —Å–∞–π—Ç–µ –ø—Ä–æ–µ–∫—Ç—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ PROYECTOS –±–ª–æ–≥–∞
        projects = []
        soup = self.fetch_page(f"{self.base_url}/blog/proyectos")
        
        if not soup:
            print("  ‚ÑπÔ∏è  –ü—Ä–æ–µ–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–µ–∫—Ç—ã
        for link in soup.find_all('a', href=lambda x: x and '/blog/' in x):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            if not href or len(text) < 5 or href.endswith('/blog/proyectos'):
                continue
            
            if not href.startswith('http'):
                href = urljoin(self.base_url, href)
            
            if any(p['source_url'] == href for p in projects):
                continue
            
            print(f"  üìê –ü—Ä–æ–µ–∫—Ç: {text[:50]}")
            
            images = []
            description = ""
            
            project_soup = self.fetch_page(href)
            if project_soup:
                img = project_soup.find('img')
                if img:
                    src = img.get('src', '')
                    img_path = self.download_image(src)
                    if img_path:
                        images.append(img_path)
                
                p_tag = project_soup.find('p')
                if p_tag:
                    description = p_tag.get_text(strip=True)[:300]
            
            projects.append({
                'title': text,
                'description': description,
                'image_url': images[0] if images else None,
                'source_url': href
            })
            
            time.sleep(0.1)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–µ–∫—Ç–æ–≤: {len(projects)}")
        return projects
    
    def extract_blog_posts(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞ Halcon Ceramicas"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ Halcon Ceramicas...")
        
        blog_posts = []
        soup = self.fetch_page(f"{self.base_url}/blog")
        
        if not soup:
            print("  ‚ÑπÔ∏è  –ë–ª–æ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        # –ò—â–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã –±–ª–æ–≥–∞
        for link in soup.find_all('a', href=lambda x: x and '/blog/' in x):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
            if not href or len(text) < 5 or href.endswith('/blog') or href.endswith('/blog/'):
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (ferias, novedades, proyectos, –Ωoticias)
            if '/blog/ferias' in href or '/blog/novedades' in href or '/blog/proyectos' in href or '/blog/noticias' in href:
                continue
            
            if not href.startswith('http'):
                href = urljoin(self.base_url, href)
            
            if any(b['source_url'] == href for b in blog_posts):
                continue
            
            print(f"  üìù –°—Ç–∞—Ç—å—è: {text[:50]}")
            
            images = []
            description = ""
            
            post_soup = self.fetch_page(href)
            if post_soup:
                img = post_soup.find('img')
                if img:
                    src = img.get('src', '')
                    img_path = self.download_image(src)
                    if img_path:
                        images.append(img_path)
                
                p_tag = post_soup.find('p')
                if p_tag:
                    description = p_tag.get_text(strip=True)[:300]
            
            blog_posts.append({
                'title': text,
                'description': description,
                'image_url': images[0] if images else None,
                'source_url': href,
                'published': True
            })
            
            time.sleep(0.1)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(blog_posts)}")
        return blog_posts


class RocedParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Roced (–ò—Å–ø–∞–Ω–∏—è)"""
    
    def __init__(self):
        super().__init__('https://roced.es', 'roced')
    
    def extract_collections(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Roced"""
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Roced...")
        soup = self.fetch_page(f"{self.base_url}/productos/")
        if not soup:
            return []
        
        collections = []
        
        for link in soup.find_all('a', href=lambda x: x and '/ceramica/' in x):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            if not href or len(text) < 2:
                continue
            if not href.startswith('http'):
                href = urljoin(self.base_url, href)
            if any(c['source_url'] == href for c in collections):
                continue
            
            print(f"  üîó –ö–æ–ª–ª–µ–∫—Ü–∏—è: {text}")
            images = []
            description = ""
            
            collection_soup = self.fetch_page(href)
            if collection_soup:
                img_tags = collection_soup.find_all('img')
                for img in img_tags:
                    src = img.get('src', '')
                    if src and 'logo' not in src.lower():
                        img_path = self.download_image(src)
                        if img_path:
                            images.append(img_path)
                            break
                paras = collection_soup.find_all('p')
                for p in paras:
                    text_content = p.get_text(strip=True)
                    if len(text_content) > 20:
                        description = text_content[:300]
                        break
            
            collections.append({
                'title': text,
                'description': description,
                'full_content': '',
                'technical_specs': '',
                'image_url': images[0] if images else None,
                'source_url': href
            })
        
        return collections
    
    def extract_collection_detail(self, url: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        return {'description': ''}
    
    def extract_projects(self) -> List[Dict]:
        return []
    
    def extract_blog_posts(self) -> List[Dict]:
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥–∞ Roced...")
        soup = self.fetch_page(f"{self.base_url}/blog/")
        if not soup:
            return []
        blog_posts = []
        for link in soup.find_all('a', href=lambda x: x and '/blog/' in x):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            if not href or len(text) < 3 or href.endswith('/blog/'):
                continue
            if not href.startswith('http'):
                href = urljoin(self.base_url, href)
            if any(p['source_url'] == href for p in blog_posts):
                continue
            print(f"  üìù –°—Ç–∞—Ç—å—è: {text}")
            blog_posts.append({'title': text, 'description': '', 'full_content': '', 'image_url': None, 'source_url': href})
        return blog_posts


class TuscaniParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Tuscania (–ò—Ç–∞–ª–∏—è)"""
    def __init__(self):
        super().__init__('https://tuscaniagres.it', 'tuscania')
    def extract_collections(self) -> List[Dict]:
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Tuscania...")
        soup = self.fetch_page(f"{self.base_url}/piastrelle/")
        if not soup:
            return []
        collections = []
        seen_urls = set()
        for link in soup.find_all('a', href=lambda x: x and '/collezioni/' in x):
            href = link.get('href', '').rstrip('/')
            if not href or href in seen_urls or '/collezioni/' not in href:
                continue
            # Skip the main /collezioni/ link
            if href.endswith('/collezioni'):
                continue
            if not href.startswith('http'):
                href = urljoin(self.base_url, href)
            seen_urls.add(href)
            
            # Extract collection name from URL
            coll_name = href.rstrip('/').split('/')[-1]
            if not coll_name or len(coll_name) < 2:
                continue
            
            print(f"  üîó –ö–æ–ª–ª–µ–∫—Ü–∏—è: {coll_name}")
            images = []
            collection_soup = self.fetch_page(href)
            if collection_soup:
                img_tags = collection_soup.find_all('img')
                for img in img_tags:
                    src = img.get('src', '')
                    if src and 'logo' not in src.lower():
                        img_path = self.download_image(src)
                        if img_path:
                            images.append(img_path)
                            break
            collections.append({'title': coll_name, 'description': '', 'full_content': '', 'technical_specs': '', 'image_url': images[0] if images else None, 'source_url': href})
        return collections
    def extract_collection_detail(self, url: str) -> Dict:
        return {'description': ''}
    def extract_projects(self) -> List[Dict]:
        return []
    def extract_blog_posts(self) -> List[Dict]:
        return []


class UnicomStarkerParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Unicom Starker (–ò—Ç–∞–ª–∏—è)"""
    def __init__(self):
        super().__init__('https://www.unicomstarker.com', 'unicom-starker')
    def extract_collections(self) -> List[Dict]:
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Unicom Starker...")
        soup = self.fetch_page(f"{self.base_url}/home")
        if not soup:
            return []
        collections = []
        for link in soup.find_all('a', href=lambda x: x and ('/products' in (x or '').lower() or '/collection' in (x or '').lower())):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            if not href or len(text) < 2:
                continue
            if not href.startswith('http'):
                href = urljoin(self.base_url, href)
            if any(c['source_url'] == href for c in collections):
                continue
            print(f"  üîó –ö–æ–ª–ª–µ–∫—Ü–∏—è: {text}")
            images = []
            collection_soup = self.fetch_page(href)
            if collection_soup:
                img_tags = collection_soup.find_all('img')
                for img in img_tags:
                    src = img.get('src', '')
                    if src and 'logo' not in src.lower():
                        img_path = self.download_image(src)
                        if img_path:
                            images.append(img_path)
                            break
            collections.append({'title': text, 'description': '', 'full_content': '', 'technical_specs': '', 'image_url': images[0] if images else None, 'source_url': href})
        return collections
    def extract_collection_detail(self, url: str) -> Dict:
        return {'description': ''}
    def extract_projects(self) -> List[Dict]:
        return []
    def extract_blog_posts(self) -> List[Dict]:
        return []


class GazziniParser(BaseManufacturerParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Gazzini (–ò—Ç–∞–ª–∏—è)"""
    def __init__(self):
        super().__init__('https://www.ceramicagazzini.it', 'gazzini')
        # Hardcoded collection list since the site blocks automated 403 on /collezioni/ listing
        self.collections_data = [
            ('amalfi-lux', 'Amalfi Lux'),
            ('antique-portofino', 'Antique Portofino'),
            ('artwork', 'Artwork'),
            ('atelier', 'Atelier'),
            ('atlantic-blue', 'Atlantic Blue'),
            ('avenue-white', 'Avenue White'),
            ('blauwsteen', 'Blauwsteen'),
            ('briques', 'Briques'),
            ('calacatta-emerald', 'Calacatta Emerald'),
            ('calacatta-oro', 'Calacatta Oro'),
        ]
    
    def extract_collections(self) -> List[Dict]:
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–π Gazzini...")
        collections = []
        for slug, name in self.collections_data:
            url = f"{self.base_url}/collezioni/{slug}/"
            print(f"  üîó –ö–æ–ª–ª–µ–∫—Ü–∏—è: {name}")
            images = []
            collection_soup = self.fetch_page(url)
            if collection_soup:
                img_tags = collection_soup.find_all('img')
                for img in img_tags:
                    src = img.get('src', '')
                    if src and 'logo' not in src.lower():
                        img_path = self.download_image(src)
                        if img_path:
                            images.append(img_path)
                            break
            collections.append({'title': name, 'description': '', 'full_content': '', 'technical_specs': '', 'image_url': images[0] if images else None, 'source_url': url})
        return collections
    def extract_collection_detail(self, url: str) -> Dict:
        return {'description': ''}
    def extract_projects(self) -> List[Dict]:
        return []
    def extract_blog_posts(self) -> List[Dict]:
        return []


# –§–∞–±—Ä–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–æ–≤
class ManufacturerParserFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π"""
    
    @staticmethod
    def get_parser(manufacturer_slug: str) -> Optional[BaseManufacturerParser]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä—Å–µ—Ä –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è"""
        parsers = {
            'aparici': ApariciParser,
            'dune': DuneParser,
            'equipe': EquipeParser,
            'ape': ApeParser,
            'lafabbrica': LaFabbricaParser,
            'baldocer': BaldocerParser,
            'casalgrande': CasalgrandeParser,
            'distrimat': DistrimatParser,
            'estudi-ceramico': EstudiCeremicoParser,
            'etile': EtileParser,
            'exagres': ExagresParser,
            'gazzini': GazziniParser,
            'halcon': HalconParser,
            'roced': RocedParser,
            'tuscania': TuscaniParser,
            'unicom-starker': UnicomStarkerParser,
            # –î–æ–±–∞–≤–ª—è–π—Ç–µ –Ω–æ–≤—ã–µ –ø–∞—Ä—Å–µ—Ä—ã –∑–¥–µ—Å—å
        }
        
        parser_class = parsers.get(manufacturer_slug)
        if parser_class:
            return parser_class()
        else:
            print(f"‚ö†Ô∏è  –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è {manufacturer_slug} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π")
            return None
