"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —Å–∞–π—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –ø–ª–∏—Ç–∫–∏.
–ü–∞—Ä—Å–∏—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –ø—Ä–æ–µ–∫—Ç—ã –∏ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∞–π—Ç–∞.
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
from typing import Dict, List, Optional
import json
import os
from urllib.parse import urljoin, urlparse
from werkzeug.utils import secure_filename
import hashlib

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
from app.services.manufacturer_parsers import ManufacturerParserFactory


class ContentScraperService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —Å–∞–π—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –∫–µ—Ä–∞–º–∏—á–µ—Å–∫–æ–π –ø–ª–∏—Ç–∫–∏.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–æ–≤ –∏ —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
    """
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
        self.manufacturer_configs = {
            'aparici': {
                'base_url': 'https://www.aparici.com/de',
                'collections_path': '/kollektionen',
                'projects_path': '/projekte',
                'blog_path': '/blog'
            },
            'ape': {
                'base_url': 'https://www.apegrupo.com/de',
                'collections_path': '/produkte',
                'projects_path': '/projekte'
            },
            'lafabbrica': {
                'base_url': 'https://www.lafabbrica.it/de',
                'collections_path': '/collections',
                'projects_path': '/projects'
            },
            'dune': {
                'base_url': 'https://duneceramics.com/de',
                'collections_path': '/serien',
                'projects_path': '/projekte',
                'blog_path': '/blog'
            },
            'equipe': {
                'base_url': 'https://www.equipeceramicas.com/de',
                'collections_path': '/kollektionen',
                'projects_path': '/projekte',
                'news_path': '/news'
            },
            'casalgrande': {
                'base_url': 'https://www.casalgrandepadana.de',
                'collections_path': '/produkte',
                'projects_path': '/realisierte-projekte',
                'magazine_path': '/magazine'
            },
            'baldocer': {
                'base_url': 'https://baldocer.com',
                'collections_path': '/producto/novedad',
                'projects_path': '/proyectos'
            },
            'distrimat': {
                'base_url': 'https://www.distrimat.es/en',
                'collections_path': '/collections',
                'projects_path': '/projects'
            },
            'estudi-ceramico': {
                'base_url': 'https://eceramico.com/en',
                'collections_path': '/collections',
                'projects_path': '/projects'
            },
            'etile': {
                'base_url': 'https://de.etile.es',
                'collections_path': '/kollektionen',
                'projects_path': '/projekte'
            },
            'exagres': {
                'base_url': 'https://www.exagres.es/en',
                'collections_path': '/collections',
                'projects_path': '/projects'
            },
            'gazzini': {
                'base_url': 'https://www.ceramicagazzini.it/de',
                'collections_path': '/kollektionen',
                'projects_path': '/projekte'
            },
            'halcon': {
                'base_url': 'https://www.halconceramicas.com',
                'collections_path': '/colecciones',
                'projects_path': '/ambientes',
                'blog_path': '/blog'
            },
            'novoceram': {
                'base_url': 'https://www.novoceram.fr',
                'collections_path': '/carrelage/collections',
                'projects_path': '/realisations',
                'blog_path': '/blog'
            },
            'roced': {
                'base_url': 'https://roced.es',
                'collections_path': '/colecciones',
                'projects_path': '/proyectos'
            },
            'tuscania': {
                'base_url': 'https://tuscaniagres.it',
                'collections_path': '/collezioni',
                'projects_path': '/progetti'
            },
            'unicom-starker': {
                'base_url': 'https://www.unicomstarker.com/home',
                'collections_path': '/collections',
                'projects_path': '/projects'
            }
        }
    
    def fetch_page(self, url: str) -> Optional[BeautifulSoup]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç BeautifulSoup –æ–±—ä–µ–∫—Ç."""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}")
            return None
    
    def normalize_url(self, url: str, base_url: str) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π."""
        if not url:
            return ''
        return urljoin(base_url, url)
    
    def download_image(self, image_url: str, manufacturer_slug: str) -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ."""
        if not image_url:
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –≤–∞–ª–∏–¥–µ–Ω
        if not image_url.startswith('http'):
            print(f"  ‚ö†Ô∏è  –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_url}")
            return None
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            url_hash = hashlib.md5(image_url.encode()).hexdigest()[:10]
            ext = os.path.splitext(urlparse(image_url).path)[1] or '.jpg'
            # –£–±–∏—Ä–∞–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            ext = ext.split('?')[0]
            if not ext or len(ext) > 5:
                ext = '.jpg'
            filename = f"{manufacturer_slug}_{url_hash}{ext}"
            
            # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            upload_dir = os.path.join('app', 'static', 'uploads', 'manufacturers')
            os.makedirs(upload_dir, exist_ok=True)
            
            filepath = os.path.join(upload_dir, filename)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∞–Ω–æ –ª–∏ —É–∂–µ
            if os.path.exists(filepath):
                print(f"  ‚ÑπÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {filename}")
                return f'manufacturers/{filename}'
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            print(f"  ‚¨áÔ∏è  –°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {image_url[:80]}...")
            response = requests.get(image_url, headers=self.headers, timeout=15, stream=True)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_type = response.headers.get('content-type', '')
            if 'image' not in content_type:
                print(f"  ‚ö†Ô∏è  –ù–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {content_type}")
                return None
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
            return f'manufacturers/{filename}'
        except requests.exceptions.RequestException as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {image_url}: {str(e)}")
            return None
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_url}: {str(e)}")
            return None
    
    def extract_full_content(self, url: str, manufacturer_slug: str) -> Dict[str, str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞.
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
            manufacturer_slug: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            
        Returns:
            Dict —Å –ø–æ–ª—è–º–∏ full_content, technical_specs
        """
        soup = self.fetch_page(url)
        if not soup:
            return {'full_content': '', 'technical_specs': ''}
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Ç–µ–≥–∞–º
        unwanted_tags = ['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'noscript']
        for tag in unwanted_tags:
            for element in soup.find_all(tag):
                element.decompose()
        
        # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –∫–ª–∞—Å—Å–∞–º –∏ ID (–Ω–∞–≤–∏–≥–∞—Ü–∏—è, –º–µ–Ω—é, —Ñ–æ—Ä–º—ã, –±–æ–∫–æ–≤—ã–µ –ø–∞–Ω–µ–ª–∏)
        unwanted_patterns = [
            'menu', 'navigation', 'nav', 'sidebar', 'side-bar', 'widget',
            'breadcrumb', 'search', 'form', 'modal', 'popup', 'cookie',
            'social', 'share', 'comment', 'footer', 'header', 'top-bar',
            'banner', 'advert', 'promo', 'newsletter', 'subscribe'
        ]
        
        for pattern in unwanted_patterns:
            # –£–¥–∞–ª—è–µ–º –ø–æ –∫–ª–∞—Å—Å—É
            for element in soup.find_all(class_=re.compile(pattern, re.I)):
                element.decompose()
            # –£–¥–∞–ª—è–µ–º –ø–æ ID
            for element in soup.find_all(id=re.compile(pattern, re.I)):
                element.decompose()
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç - –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find('div', class_=re.compile(r'(^|\s)(content|main|article|entry|post|detail|product|project)(\s|$)', re.I)) or
            soup.find('div', id=re.compile(r'(content|main|article)', re.I))
        )
        
        if not main_content:
            # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –±–µ—Ä–µ–º body
            main_content = soup.find('body')
        
        if not main_content:
            return {'full_content': '', 'technical_specs': ''}
        
        seen_texts = set()  # –î–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        full_content_parts = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (h1)
        h1 = main_content.find('h1')
        if h1:
            title_text = h1.get_text(strip=True)
            if title_text and len(title_text) > 3:
                full_content_parts.append(f"<h1>{title_text}</h1>")
                seen_texts.add(title_text.lower())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        for h in main_content.find_all(['h2', 'h3', 'h4'], limit=15):
            text = h.get_text(strip=True)
            text_lower = text.lower()
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if text and len(text) > 5 and text_lower not in seen_texts:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏-—Å—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if not any(keyword in text_lower for keyword in ['produkte', 'kollektionen', 'kontakt', 'einloggen', 'registrieren', 'alle']):
                    tag_name = h.name
                    full_content_parts.append(f"<{tag_name}>{text}</{tag_name}>")
                    seen_texts.add(text_lower)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        paragraph_count = 0
        for p in main_content.find_all('p', limit=100):
            if paragraph_count >= 20:  # –ú–∞–∫—Å–∏–º—É–º 20 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                break
                
            text = p.get_text(strip=True)
            text_lower = text.lower()
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º:
            # - –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ (–º–µ–Ω—å—à–µ 50 —Å–∏–º–≤–æ–ª–æ–≤)
            # - –î—É–±–ª–∏–∫–∞—Ç—ã
            # - –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            if (text and len(text) >= 50 and text_lower not in seen_texts and
                not any(keyword in text_lower for keyword in [
                    'brauche hilfe', 'suche nach', 'cookie', 'einloggen', 
                    'registrieren', 'alle produkte', 'datenschutz', 'impressum'
                ])):
                full_content_parts.append(f"<p>{text}</p>")
                seen_texts.add(text_lower)
                paragraph_count += 1
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–∫–∏ (—Ç–æ–ª—å–∫–æ –Ω–µ–±–æ–ª—å—à–∏–µ, —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ)
        list_count = 0
        for ul in main_content.find_all(['ul', 'ol'], limit=5):
            if list_count >= 3:  # –ú–∞–∫—Å–∏–º—É–º 3 —Å–ø–∏—Å–∫–∞
                break
                
            items = []
            for li in ul.find_all('li', limit=10):
                text = li.get_text(strip=True)
                text_lower = text.lower()
                # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã
                if (text and len(text) >= 10 and len(text) <= 200 and 
                    text_lower not in seen_texts and
                    not any(keyword in text_lower for keyword in ['produkte', 'kollektionen', 'kontakt'])):
                    items.append(f"<li>{text}</li>")
                    seen_texts.add(text_lower)
            
            if len(items) >= 2:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞
                list_tag = ul.name
                full_content_parts.append(f"<{list_tag}>{''.join(items)}</{list_tag}>")
                list_count += 1
        
        # –ò—â–µ–º –∏ —Å–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ)
        seen_images = set()
        for img in main_content.find_all('img', limit=10):
            img_url = img.get('src') or img.get('data-src')
            if img_url and img_url not in seen_images:
                seen_images.add(img_url)
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                img_url = self.normalize_url(img_url, url)
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ª–æ–≥–æ—Ç–∏–ø—ã, –∏–∫–æ–Ω–∫–∏)
                width = img.get('width', '0')
                if width and width.isdigit() and int(width) < 100:
                    continue
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                local_path = self.download_image(img_url, manufacturer_slug)
                if local_path:
                    alt_text = img.get('alt', 'Product image')
                    full_content_parts.append(f'<img src="/static/uploads/{local_path}" alt="{alt_text}" class="img-fluid my-3">')
        
        # –ò—â–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        technical_specs = ""
        specs_keywords = ['spec', 'technical', '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏', '–¥–∞–Ω–Ω—ã–µ', 'details', 'properties', 'features', 'formato', 'acabado']
        
        for keyword in specs_keywords:
            specs_section = soup.find(['div', 'section', 'table'], class_=re.compile(keyword, re.I))
            if specs_section:
                specs_items = []
                
                # –ï—Å–ª–∏ —ç—Ç–æ —Ç–∞–±–ª–∏—Ü–∞
                if specs_section.name == 'table':
                    for row in specs_section.find_all('tr', limit=20):
                        cells = row.find_all(['td', 'th'])
                        if len(cells) >= 2:
                            label = cells[0].get_text(strip=True)
                            value = cells[1].get_text(strip=True)
                            if label and value and len(label) < 100:
                                specs_items.append(f"{label}: {value}")
                else:
                    # –ò—â–µ–º –ø–∞—Ä—ã –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ
                    for item in specs_section.find_all(['li', 'p', 'div'], limit=20):
                        text = item.get_text(strip=True)
                        if text and 10 < len(text) < 200:
                            specs_items.append(text)
                
                if specs_items:
                    technical_specs = "\n".join(specs_items[:15])  # –ú–∞–∫—Å–∏–º—É–º 15 —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
                    break
        
        full_content = "\n".join(full_content_parts)
        
        return {
            'full_content': full_content,
            'technical_specs': technical_specs
        }
    
    def extract_collections(self, manufacturer_slug: str) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–ª–∏—Ç–∫–∏ —Å —Å–∞–π—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è.
        
        Args:
            manufacturer_slug: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è (aparici, dune, etc.)
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –∫–æ–ª–ª–µ–∫—Ü–∏–π —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º, –æ–ø–∏—Å–∞–Ω–∏–µ–º, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        """
        config = self.manufacturer_configs.get(manufacturer_slug)
        if not config:
            return []
        
        url = config['base_url'] + config.get('collections_path', '')
        soup = self.fetch_page(url)
        
        if not soup:
            return []
        
        collections = []
        seen_urls = set()  # –î–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collection_links = (
            soup.find_all('a', href=re.compile(r'/(collections?|serien|kollektionen?|produkt|colecciones|collezioni)/')) +
            soup.find_all('a', class_=re.compile(r'collection|product|serie', re.I))
        )
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(collection_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è {manufacturer_slug}")
        
        for link in collection_links[:15]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            # –§–æ—Ä–º–∏—Ä—É–µ–º source_url
            href = link.get('href', '')
            source_url = self.normalize_url(href, config['base_url'])
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ URL
            if source_url in seen_urls or not source_url or not source_url.startswith('http'):
                continue
            seen_urls.add(source_url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ø—Ä–µ–≤—å—é
            title = link.get_text(strip=True)
            if not title or len(title) < 3:
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
                title_elem = link.find(['h2', 'h3', 'h4', 'h5', 'span', 'div'])
                if title_elem:
                    title = title_elem.get_text(strip=True)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not title or len(title) < 3 or len(title) > 100:
                continue
            
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {title} - {source_url}")
            
            # –ò—â–µ–º –ø—Ä–µ–≤—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å–ø–∏—Å–∫–∞
            preview_image = None
            
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤–Ω—É—Ç—Ä–∏ —Å—Å—ã–ª–∫–∏
            img = link.find('img')
            if not img:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                parent = link.find_parent(['div', 'article', 'li'])
                if parent:
                    img = parent.find('img')
            
            if img:
                img_src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-original')
                if img_src:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                    img_src = self.normalize_url(img_src, config['base_url'])
                    # –°–∫–∞—á–∏–≤–∞–µ–º –ø—Ä–µ–≤—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    preview_image = self.download_image(img_src, manufacturer_slug)
                    if preview_image:
                        print(f"  ‚úì –°–∫–∞—á–∞–Ω–æ –ø—Ä–µ–≤—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {preview_image}")
                else:
                    print(f"  ‚ö†Ô∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ, –Ω–æ src –ø—É—Å—Ç–æ–π")
            else:
                print(f"  ‚ö†Ô∏è  –ü—Ä–µ–≤—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è {title}")
            
            # –ò—â–µ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å–ø–∏—Å–∫–∞
            short_description = ""
            # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ —Å—Å—ã–ª–∫–∏
            parent = link.find_parent(['div', 'article'])
            if parent:
                desc_elem = parent.find(['p', 'div'], class_=re.compile(r'desc|subtitle|text|summary', re.I))
                if desc_elem:
                    short_description = desc_elem.get_text(strip=True)[:200]
            
            # –ó–∞—Ö–æ–¥–∏–º –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            full_data = self.extract_full_content(source_url, manufacturer_slug)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–≤—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø–æ–ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ full_content
            # (–≤ full_content —É–∂–µ –µ—Å—Ç—å —Å–∫–∞—á–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
            final_image = preview_image
            if not final_image and full_data.get('full_content'):
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ full_content
                import re as regex
                img_match = regex.search(r'<img src="([^"]+)"', full_data.get('full_content', ''))
                if img_match:
                    # –ë–µ—Ä–µ–º –ø—É—Ç—å –∏–∑ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    final_image = img_match.group(1).replace('/static/uploads/', '')
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º full_content –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è, –µ—Å–ª–∏ short_description –ø—É—Å—Ç–æ–µ
            final_description = short_description
            if not final_description and full_data.get('full_content'):
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ HTML —Ç–µ–≥–æ–≤)
                import re as regex
                text_only = regex.sub(r'<[^>]+>', '', full_data.get('full_content', ''))
                final_description = text_only[:300].strip()
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            collections.append({
                'title': title,
                'description': final_description,
                'full_content': full_data.get('full_content', ''),
                'technical_specs': full_data.get('technical_specs', ''),
                'image_url': final_image,  # –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None
                'source_url': source_url
            })
            
            print(f"  ‚úì –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–æ–±–∞–≤–ª–µ–Ω–∞: {title}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {final_image or '–Ω–µ—Ç'}\n")
        
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(collections)} –∫–æ–ª–ª–µ–∫—Ü–∏–π –¥–ª—è {manufacturer_slug}")
        return collections
    
    def extract_projects(self, manufacturer_slug: str) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã/—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å —Å–∞–π—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è.
        
        Args:
            manufacturer_slug: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º, –æ–ø–∏—Å–∞–Ω–∏–µ–º, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        """
        config = self.manufacturer_configs.get(manufacturer_slug)
        if not config or 'projects_path' not in config:
            return []
        
        url = config['base_url'] + config['projects_path']
        soup = self.fetch_page(url)
        
        if not soup:
            return []
        
        projects = []
        seen_urls = set()
        
        # –ò—â–µ–º –ø—Ä–æ–µ–∫—Ç—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        project_cards = soup.find_all(['article', 'div', 'a'], class_=re.compile(r'project|realiz|card|proyecto'))
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–µ–∫—Ç—ã
        project_links = soup.find_all('a', href=re.compile(r'/(project|proyecto|realiz|references?)/'))
        all_elements = list(project_cards) + list(project_links)
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ–≤ –¥–ª—è {manufacturer_slug}")
        
        for element in all_elements[:12]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
            if element.name == 'a':
                title = element.get_text(strip=True)
                link_elem = element
            else:
                title_elem = element.find(['h2', 'h3', 'h4', 'h5'])
                title = title_elem.get_text(strip=True) if title_elem else None
                link_elem = element.find('a')
            
            if not title or len(title) < 3:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            source_url = ""
            if link_elem:
                href = link_elem.get('href', '')
                source_url = self.normalize_url(href, config['base_url'])
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Å—ã–ª–∫–∏
            if not source_url or source_url in seen_urls:
                continue
            seen_urls.add(source_url)
            
            # –ò—â–µ–º –ø—Ä–µ–≤—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = element.find('img')
            preview_image = None
            if img:
                img_src = img.get('src') or img.get('data-src')
                if img_src:
                    img_src = self.normalize_url(img_src, config['base_url'])
                    preview_image = self.download_image(img_src, manufacturer_slug)
            
            # –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            desc_elem = element.find('p')
            short_description = desc_elem.get_text(strip=True)[:200] if desc_elem else ""
            
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞: {title}")
            
            # –ó–∞—Ö–æ–¥–∏–º –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            full_data = self.extract_full_content(source_url, manufacturer_slug)
            
            projects.append({
                'title': title,
                'description': short_description if short_description else full_data.get('full_content', '')[:300],
                'full_content': full_data.get('full_content', ''),
                'technical_specs': full_data.get('technical_specs', ''),
                'image_url': preview_image,
                'source_url': source_url
            })
        
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(projects)} –ø—Ä–æ–µ–∫—Ç–æ–≤ –¥–ª—è {manufacturer_slug}")
        return projects
    
    def extract_blog_posts(self, manufacturer_slug: str) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ –±–ª–æ–≥–∞/–Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è.
        
        Args:
            manufacturer_slug: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º, –∞–Ω–æ–Ω—Å–æ–º, –¥–∞—Ç–æ–π, –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        """
        config = self.manufacturer_configs.get(manufacturer_slug)
        if not config:
            return []
        
        blog_path = config.get('blog_path') or config.get('news_path') or config.get('magazine_path')
        if not blog_path:
            return []
        
        url = config['base_url'] + blog_path
        soup = self.fetch_page(url)
        
        if not soup:
            return []
        
        posts = []
        seen_urls = set()
        
        # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        post_cards = soup.find_all(['article', 'div'], class_=re.compile(r'post|article|blog|news|noticia'))
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
        post_links = soup.find_all('a', href=re.compile(r'/(post|article|blog|news|noticia)/'))
        all_elements = list(post_cards) + list(post_links)
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –±–ª–æ–≥–∞ –¥–ª—è {manufacturer_slug}")
        
        for element in all_elements[:8]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
            if element.name == 'a':
                title = element.get_text(strip=True)
                link_elem = element
            else:
                title_elem = element.find(['h2', 'h3', 'h4'])
                title = title_elem.get_text(strip=True) if title_elem else None
                link_elem = element.find('a')
            
            if not title or len(title) < 5:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
            source_url = ""
            if link_elem:
                href = link_elem.get('href', '')
                source_url = self.normalize_url(href, config['base_url'])
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if not source_url or source_url in seen_urls:
                continue
            seen_urls.add(source_url)
            
            # –ò—â–µ–º –ø—Ä–µ–≤—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = element.find('img')
            preview_image = None
            if img:
                img_src = img.get('src') or img.get('data-src')
                if img_src:
                    img_src = self.normalize_url(img_src, config['base_url'])
                    preview_image = self.download_image(img_src, manufacturer_slug)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–Ω–æ–Ω—Å
            desc_elem = element.find('p')
            short_content = desc_elem.get_text(strip=True)[:200] if desc_elem else ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É (–µ—Å–ª–∏ –µ—Å—Ç—å)
            date_elem = element.find(['time', 'span'], class_=re.compile(r'date|time|fecha'))
            created_at = None
            if date_elem:
                try:
                    date_text = date_elem.get_text(strip=True)
                    # Placeholder - –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞—Ç
                    created_at = datetime.now()
                except:
                    pass
            
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ –±–ª–æ–≥–∞: {title}")
            
            # –ó–∞—Ö–æ–¥–∏–º –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            full_data = self.extract_full_content(source_url, manufacturer_slug)
            
            posts.append({
                'title': title,
                'content': short_content if short_content else full_data.get('full_content', '')[:300],
                'full_content': full_data.get('full_content', ''),
                'image_url': preview_image,
                'source_url': source_url,
                'created_at': created_at
            })
        
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(posts)} —Å—Ç–∞—Ç–µ–π –±–ª–æ–≥–∞ –¥–ª—è {manufacturer_slug}")
        return posts
    
    def extract_all_content(self, manufacturer_slug: str) -> Dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Å–∞–π—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è (–∫–æ–ª–ª–µ–∫—Ü–∏–∏, –ø—Ä–æ–µ–∫—Ç—ã, –±–ª–æ–≥).
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ –±–∞–∑–æ–≤—ã–π.
        
        Args:
            manufacturer_slug: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
            
        Returns:
            Dict: –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ collections, projects, blog_posts
        """
        print(f"\n==== –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è {manufacturer_slug} ====")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
        custom_parser = ManufacturerParserFactory.get_parser(manufacturer_slug)
        
        if custom_parser:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
            print(f"‚ú® –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è {manufacturer_slug}")
            try:
                return {
                    'collections': custom_parser.extract_collections(),
                    'projects': custom_parser.extract_projects(),
                    'blog_posts': custom_parser.extract_blog_posts()
                }
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–º –ø–∞—Ä—Å–µ—Ä–µ: {str(e)}")
                print("‚ö†Ô∏è  –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –±–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä
        print(f"üìù –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è {manufacturer_slug}")
        return {
            'collections': self.extract_collections(manufacturer_slug),
            'projects': self.extract_projects(manufacturer_slug),
            'blog_posts': self.extract_blog_posts(manufacturer_slug)
        }
    
    def get_manufacturer_info(self, manufacturer_slug: str) -> Optional[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        
        Args:
            manufacturer_slug: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
            
        Returns:
            Dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ (–Ω–∞–∑–≤–∞–Ω–∏–µ, –æ–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç—Ä–∞–Ω–∞)
        """
        config = self.manufacturer_configs.get(manufacturer_slug)
        if not config:
            return None
        
        soup = self.fetch_page(config['base_url'])
        if not soup:
            return None
        
        # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
        about_section = soup.find(['section', 'div'], class_=re.compile(r'about|company|firma'))
        description = ""
        if about_section:
            desc_elem = about_section.find('p')
            if desc_elem:
                description = desc_elem.get_text(strip=True)
        
        # –ò—â–µ–º –ª–æ–≥–æ—Ç–∏–ø
        logo = soup.find('img', {'alt': re.compile(r'logo', re.I)})
        logo_url = logo.get('src') if logo else None
        
        return {
            'name': manufacturer_slug.title(),
            'website': config['base_url'],
            'description': description,
            'logo_url': logo_url
        }


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
scraper_service = ContentScraperService()
