"""
–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Aparici
"""

import requests
from bs4 import BeautifulSoup

def analyze_collection_page():
    # –ü—Ä–∏–º–µ—Ä URL –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    url = 'https://www.aparici.com/de/kollektionen/ibiza'
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    print(f"üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {url}\n")
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print("=" * 60)
        print("1. –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–ø–µ—Ä–≤—ã–µ 10):")
        print("=" * 60)
        
        images = soup.find_all('img')
        for i, img in enumerate(images[:10], 1):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy')
            print(f"{i}. Src: {src[:80] if src else 'no src'}")
            print(f"   Alt: {img.get('alt', 'no alt')}")
            print(f"   Classes: {img.get('class', [])}")
            print()
        
        print("\n" + "=" * 60)
        print("2. –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")
        print("=" * 60)
        
        h1 = soup.find('h1')
        if h1:
            print(f"H1: {h1.get_text(strip=True)}")
            print(f"Classes: {h1.get('class', [])}")
        
        print("\n" + "=" * 60)
        print("3. –û–ø–∏—Å–∞–Ω–∏–µ/–∫–æ–Ω—Ç–µ–Ω—Ç:")
        print("=" * 60)
        
        # –ò—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = soup.find_all('p')
        for i, p in enumerate(paragraphs[:5], 1):
            text = p.get_text(strip=True)
            if len(text) > 30:
                print(f"{i}. {text[:150]}...")
                print(f"   Parent: {p.parent.name}, classes: {p.parent.get('class', [])}")
                print()
        
        print("\n" + "=" * 60)
        print("4. –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        print("=" * 60)
        
        # –ò—â–µ–º main, article –∏–ª–∏ div —Å –∫–ª–∞—Å—Å–∞–º–∏ content
        main = soup.find(['main', 'article'])
        if main:
            print(f"–ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä: {main.name}")
            print(f"Classes: {main.get('class', [])}")
        
        # –ò—â–µ–º divs —Å –∫–ª–∞—Å—Å–∞–º–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ description, content, detail
        import re
        content_divs = soup.find_all('div', class_=re.compile(r'description|content|detail|info', re.I))
        print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(content_divs)} div —Å –∫–ª–∞—Å—Å–∞–º–∏ description/content/detail/info")
        for div in content_divs[:3]:
            print(f"- Classes: {div.get('class', [])}")
            text = div.get_text(strip=True)
            if text:
                print(f"  Text: {text[:100]}...")
        
        print("\n" + "=" * 60)
        print("5. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print("=" * 60)
        
        # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
        tables = soup.find_all('table')
        if tables:
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(tables)} —Ç–∞–±–ª–∏—Ü")
            for table in tables[:2]:
                rows = table.find_all('tr')
                for row in rows[:5]:
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        print(' | '.join([cell.get_text(strip=True) for cell in cells]))
        
        specs_divs = soup.find_all(['div', 'dl'], class_=re.compile(r'spec|technical|formato|acabado|caracteristica', re.I))
        if specs_divs:
            print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(specs_divs)} –±–ª–æ–∫–æ–≤ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π")
            for div in specs_divs[:2]:
                print(f"Classes: {div.get('class', [])}")
                print(f"Text: {div.get_text(strip=True)[:200]}...")
                print()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")

if __name__ == '__main__':
    analyze_collection_page()
