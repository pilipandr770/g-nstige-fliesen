"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü APE Grupo
"""
import requests
from bs4 import BeautifulSoup

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

base_url = 'https://www.apegrupo.com'

print("="*80)
print("–î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ APE GRUPO")
print("="*80)

# 1. –ö–û–õ–õ–ï–ö–¶–ò–ò
print("\nüì¶ –ö–û–õ–õ–ï–ö–¶–ò–ò (/de/produkte)")
print("-"*80)
try:
    response = requests.get(base_url + '/de/produkte', headers=headers, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏
    containers = soup.find_all(['div', 'section', 'article'], class_=lambda x: x and ('product' in str(x).lower() or 'collection' in str(x).lower() or 'serie' in str(x).lower()))
    
    if containers:
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(containers)} –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤")
        for i, container in enumerate(containers[:3], 1):
            print(f"\n–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä {i}:")
            print(f"  –¢–µ–≥: {container.name}")
            print(f"  –ö–ª–∞—Å—Å: {container.get('class')}")
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É
            link = container.find('a', href=True)
            if link:
                print(f"  –°—Å—ã–ª–∫–∞: {link.get('href')}")
                print(f"  –¢–µ–∫—Å—Ç: {link.get_text(strip=True)[:50]}")
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = container.find('img')
            if img:
                print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img.get('src')}")
                print(f"  Alt: {img.get('alt')}")
    else:
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ - –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã
        all_links = soup.find_all('a', href=lambda x: x and '/produkte/' in x)
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã")
        for link in all_links[:5]:
            print(f"  ‚Ä¢ {link.get('href')} - {link.get_text(strip=True)[:30]}")
            
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

# 2. –ü–†–û–ï–ö–¢–´
print("\nüèóÔ∏è –ü–†–û–ï–ö–¢–´ (/de/projekte)")
print("-"*80)
try:
    response = requests.get(base_url + '/de/projekte', headers=headers, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –ø—Ä–æ–µ–∫—Ç–∞–º–∏
    containers = soup.find_all(['div', 'article'], class_=lambda x: x and ('project' in str(x).lower() or 'work' in str(x).lower() or 'reference' in str(x).lower()))
    
    if containers:
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(containers)} –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ–≤")
        for i, container in enumerate(containers[:3], 1):
            print(f"\n–ü—Ä–æ–µ–∫—Ç {i}:")
            print(f"  –ö–ª–∞—Å—Å: {container.get('class')}")
            
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = container.find(['h1', 'h2', 'h3', 'h4'])
            if title:
                print(f"  –ù–∞–∑–≤–∞–Ω–∏–µ: {title.get_text(strip=True)}")
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É
            link = container.find('a', href=True)
            if link:
                print(f"  –°—Å—ã–ª–∫–∞: {link.get('href')}")
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = container.find('img')
            if img:
                print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img.get('src')}")
    else:
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
        all_links = soup.find_all('a', href=lambda x: x and '/projekte/' in x)
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—Ä–æ–µ–∫—Ç—ã")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –∫–ª–∞—Å—Å—ã div –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        all_divs = soup.find_all('div', class_=True, limit=30)
        classes_set = set()
        for div in all_divs:
            classes = div.get('class', [])
            if isinstance(classes, list):
                classes_set.update(classes)
        
        print(f"\n–í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–ø–µ—Ä–≤—ã–µ 20):")
        for cls in sorted(list(classes_set))[:20]:
            print(f"  ‚Ä¢ {cls}")
            
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

# 3. –ë–õ–û–ì
print("\nüì∞ –ë–õ–û–ì (/de/blog)")
print("-"*80)
try:
    response = requests.get(base_url + '/de/blog', headers=headers, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏
    articles = soup.find_all(['article', 'div'], class_=lambda x: x and ('blog' in str(x).lower() or 'post' in str(x).lower() or 'news' in str(x).lower()))
    
    if articles:
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
        for i, article in enumerate(articles[:3], 1):
            print(f"\n–°—Ç–∞—Ç—å—è {i}:")
            print(f"  –ö–ª–∞—Å—Å: {article.get('class')}")
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = article.find(['h1', 'h2', 'h3'])
            if title:
                print(f"  –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title.get_text(strip=True)[:50]}")
            
            # –°—Å—ã–ª–∫–∞
            link = article.find('a', href=True)
            if link:
                print(f"  –°—Å—ã–ª–∫–∞: {link.get('href')}")
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = article.find('img')
            if img:
                print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img.get('src')}")
    else:
        all_links = soup.find_all('a', href=lambda x: x and '/blog/' in x)
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –±–ª–æ–≥")
        
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

print("\n" + "="*80)
